{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import lightgbm as lgb\n",
    "\n",
    "from os.path import join as pjoin\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "from keras.layers import Dense, Input, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 128\n",
    "datadir = 'data'\n",
    "dataset_size=1e6\n",
    "filename = 'Base{}.txt'.format(base)\n",
    "train_split=0.8\n",
    "val_split=0.1\n",
    "test_split=0.1\n",
    "batch_size=500\n",
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(prepare=True):\n",
    "    data = []\n",
    "    for row in open(pjoin(datadir, filename)):\n",
    "        if len(row.split()) == 2:\n",
    "            inp, target = row.split()\n",
    "            data.append([np.array([int(x) if prepare else float(x) for x in inp]), float(target)])\n",
    "    if prepare: # Magick preprocessing\n",
    "        new_data = []\n",
    "        size = len(data)\n",
    "        for x, y in data:\n",
    "            s = np.zeros_like(x)\n",
    "            lamb = reduce(lambda a, b: a^b, x, 0)\n",
    "            for i in range(len(x)):\n",
    "                s[i] = float(-1 if lamb == 1 else 1)\n",
    "                lamb ^= x[i]\n",
    "            new_data.append([s, y])\n",
    "        data = new_data\n",
    "    random.shuffle(data)\n",
    "    train_count = int(dataset_size*train_split)\n",
    "    val_count = int(dataset_size*val_split)\n",
    "    test_count = int(dataset_size*test_split)\n",
    "    return data[:train_count], data[train_count:train_count+val_count], \\\n",
    "           data[train_count+val_count:train_count+val_count+test_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = list(map(np.array, zip(*train_data)))\n",
    "x_val, y_val = list(map(np.array, zip(*test_data)))\n",
    "x_test, y_test = list(map(np.array, zip(*test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -1  1 ...  1 -1 -1]\n",
      " [-1  1  1 ... -1 -1 -1]\n",
      " [ 1 -1  1 ... -1 -1  1]\n",
      " ...\n",
      " [-1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ...  1  1 -1]\n",
      " [ 1 -1  1 ... -1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model():\n",
    "    optimizer = RMSprop(lr)\n",
    "    inp = Input((base,))\n",
    "    x = Dense(base//2, activation='relu')(inp)\n",
    "    x = Dense(base//4, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inp, x)\n",
    "    model.compile(loss='binary_crossentropy', metrics=['binary_accuracy'], optimizer=optimizer)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.9, min_lr=1e-4):\n",
    "    def schedule(epoch):\n",
    "        return max(min_lr, initial_lr * (decay_factor ** (epoch)))\n",
    "    \n",
    "    return LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_anneal_schedule(t, alpha_zero=1e-2):\n",
    "    T, M = 80, 10\n",
    "    cos_inner = np.pi * ((t+1) % (T // M))\n",
    "    cos_inner /= T // M\n",
    "    cos_out = np.cos(cos_inner) + 1\n",
    "    return max(float(alpha_zero / 2 * cos_out), alpha_zero / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 100000 samples\n",
      "Epoch 1/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 3.1630 - binary_accuracy: 0.6052 - val_loss: 0.3358 - val_binary_accuracy: 0.8910\n",
      "Epoch 2/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.2057 - binary_accuracy: 0.9492 - val_loss: 0.0640 - val_binary_accuracy: 0.9734\n",
      "Epoch 3/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.1468 - binary_accuracy: 0.9628 - val_loss: 0.0440 - val_binary_accuracy: 0.9809\n",
      "Epoch 4/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0848 - binary_accuracy: 0.9717 - val_loss: 0.0616 - val_binary_accuracy: 0.9611\n",
      "Epoch 5/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.1035 - binary_accuracy: 0.9708 - val_loss: 0.0409 - val_binary_accuracy: 0.9820\n",
      "Epoch 6/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0591 - binary_accuracy: 0.9784 - val_loss: 0.0614 - val_binary_accuracy: 0.9797\n",
      "Epoch 7/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0718 - binary_accuracy: 0.9788 - val_loss: 0.0360 - val_binary_accuracy: 0.9840\n",
      "Epoch 8/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0663 - binary_accuracy: 0.9798 - val_loss: 0.0351 - val_binary_accuracy: 0.9853\n",
      "Epoch 9/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0763 - binary_accuracy: 0.9793 - val_loss: 0.0404 - val_binary_accuracy: 0.9801\n",
      "Epoch 10/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0403 - binary_accuracy: 0.9838 - val_loss: 0.0390 - val_binary_accuracy: 0.9849\n",
      "Epoch 11/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0591 - binary_accuracy: 0.9817 - val_loss: 0.0337 - val_binary_accuracy: 0.9843\n",
      "Epoch 12/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0729 - binary_accuracy: 0.9819 - val_loss: 0.0264 - val_binary_accuracy: 0.9891\n",
      "Epoch 13/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0432 - binary_accuracy: 0.9852 - val_loss: 0.0486 - val_binary_accuracy: 0.9841\n",
      "Epoch 14/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0474 - binary_accuracy: 0.9852 - val_loss: 0.0399 - val_binary_accuracy: 0.9806\n",
      "Epoch 15/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0563 - binary_accuracy: 0.9843 - val_loss: 0.0315 - val_binary_accuracy: 0.9863\n",
      "Epoch 16/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0530 - binary_accuracy: 0.9848 - val_loss: 0.0266 - val_binary_accuracy: 0.9886\n",
      "Epoch 17/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0374 - binary_accuracy: 0.9863 - val_loss: 0.0375 - val_binary_accuracy: 0.9819\n",
      "Epoch 18/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0428 - binary_accuracy: 0.9839 - val_loss: 0.0798 - val_binary_accuracy: 0.9733\n",
      "Epoch 19/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0353 - binary_accuracy: 0.9860 - val_loss: 0.0267 - val_binary_accuracy: 0.9890\n",
      "Epoch 20/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0404 - binary_accuracy: 0.9867 - val_loss: 0.0265 - val_binary_accuracy: 0.9881\n",
      "Epoch 21/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0428 - binary_accuracy: 0.9860 - val_loss: 0.0262 - val_binary_accuracy: 0.9889\n",
      "Epoch 22/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0353 - binary_accuracy: 0.9872 - val_loss: 0.0336 - val_binary_accuracy: 0.9880\n",
      "Epoch 23/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0468 - binary_accuracy: 0.9861 - val_loss: 0.0245 - val_binary_accuracy: 0.9899\n",
      "Epoch 24/100\n",
      "800000/800000 [==============================] - 1s 2us/step - loss: 0.0505 - binary_accuracy: 0.9851 - val_loss: 0.0276 - val_binary_accuracy: 0.9884\n",
      "Epoch 25/100\n",
      "800000/800000 [==============================] - 2s 2us/step - loss: 0.0313 - binary_accuracy: 0.9878 - val_loss: 0.0436 - val_binary_accuracy: 0.9825\n",
      "Epoch 26/100\n",
      "680000/800000 [========================>.....] - ETA: 0s - loss: 0.0305 - binary_accuracy: 0.9878"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-431ed4c9e485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         callbacks = callbacks, verbose=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    185\u001b[0m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = nn_model()\n",
    "early_stopping_callback = EarlyStopping(monitor='loss', patience=15, min_delta=1e-4)\n",
    "lr_callback = LearningRateScheduler(cosine_anneal_schedule)\n",
    "\n",
    "lr=1e-3\n",
    "fl = False\n",
    "callbacks = [early_stopping_callback, lr_callback] if fl else [early_stopping_callback]\n",
    "model.fit(x_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=10000,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks = callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_lr(min_lr=1e-3, mult=0.99):\n",
    "    def callback(env):\n",
    "        last_lr = env.params['learning_rate']\n",
    "        env.params['learning_rate'] = last_lr*mult if last_lr > min_lr else lr\n",
    "    callback.before_iteration = True\n",
    "    callback.order = 0\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    train_dataset = lgb.Dataset(x_train, label=y_train, \\\n",
    "                         feature_name=['c{}'.format(i) for i in range(base)], \\\n",
    "                         categorical_feature=['c{}'.format(i) for i in range(base)])\n",
    "    val_dataset = lgb.Dataset(x_val, label=y_val, \\\n",
    "                             feature_name=['c{}'.format(i) for i in range(base)], \\\n",
    "                             categorical_feature=['c{}'.format(i) for i in range(base)])\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "lr=0.1\n",
    "\n",
    "param = {'num_leaves': 2047, \n",
    "         'num_trees':1000, \n",
    "         'objective':'binary', \n",
    "         'learning_rate' : lr,\n",
    "         'boosting': 'dart',\n",
    "         'max_bin': 2047}\n",
    "param['metric'] = ['auc', 'binary_logloss']\n",
    "\n",
    "train_dataset, val_dataset = get_datasets()\n",
    "\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, train_dataset, num_round, valid_sets=[val_dataset])#, \\\n",
    "                    #callbacks=[reset_lr(lr/0.1)], early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='poly', verbose=2)\n",
    "clf.fit(x_train[:50000], y_train[:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5981000000000001\n"
     ]
    }
   ],
   "source": [
    "res = clf.predict(x_test[:10000])\n",
    "print(1 - np.abs(np.array(res) - np.array(y_test[:10000])).sum() / len(y_test[:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
