{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# to prevent kernel death because of\n",
    "\n",
    "# 2019-03-30 17:11:04.317013: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n",
    "# OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.\n",
    "# OMP: Hint: This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_DATASET_DIR = '../../nmnist/notMNIST_small/'\n",
    "LARGE_DATASET_DIR = '../../nmnist/notMNIST_large/'\n",
    "CACHE = {}\n",
    "LABEL_MAP = {}\n",
    "INV_LABEL_MAP = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(data_dir, override=False):\n",
    "    f_v = 0\n",
    "    global CACHE\n",
    "    if not CACHE.get(data_dir, []) or override:\n",
    "        CACHE[data_dir] = []\n",
    "        X, y = [], []\n",
    "        for f in tqdm(os.listdir(data_dir), desc='Letter'):\n",
    "            if not f.startswith('.'):\n",
    "                img_dir = os.path.join(data_dir, f)\n",
    "                for img in os.listdir(img_dir):\n",
    "                    img_path = os.path.join(img_dir, img)\n",
    "                    data = cv2.imread(img_path, 0)\n",
    "                    if data is None:\n",
    "                        continue\n",
    "                    X.append(data * 2 / 255 - 1)\n",
    "                    if LABEL_MAP.get(f) is None:\n",
    "                        LABEL_MAP[f] = f_v\n",
    "                        INV_LABEL_MAP[f_v] = f\n",
    "                        f_v += 1\n",
    "                    y.append(LABEL_MAP[f])\n",
    "        CACHE[data_dir].append(np.array(X))\n",
    "        CACHE[data_dir].append(np.array(y))\n",
    "    return CACHE[data_dir][0], CACHE[data_dir][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir, verbose=False, override=False):\n",
    "    X, y = read(data_dir, override=override)\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    N = X.shape[0]\n",
    "    if verbose:\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        print(X[:5])\n",
    "        print(y[:5])\n",
    "        print(np.unique(y))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_data(data_dir, size=(0.7, 0.3), verbose=False, random_state=6, override=False):\n",
    "    X, y = shuffle(*get_data(data_dir, verbose=verbose, override=override), random_state=random_state)\n",
    "    assert abs(np.sum(size) - 1.0) < 0.001\n",
    "    if verbose:\n",
    "        print('splitting...')\n",
    "#     X = X[:60000]\n",
    "#     y = y[:60000]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size[1], random_state=random_state)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bffd3875214e45b059475668a94f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Letter', max=10, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(529114, 28, 28)\n",
      "(529114,)\n",
      "[[[-0.12156863 -0.83529412 -0.59215686 ... -1.         -1.\n",
      "   -1.        ]\n",
      "  [-0.11372549 -0.38823529 -0.71764706 ... -1.         -1.\n",
      "   -1.        ]\n",
      "  [-1.         -1.         -1.         ... -1.         -1.\n",
      "   -1.        ]\n",
      "  ...\n",
      "  [-1.         -1.         -1.         ... -1.         -1.\n",
      "   -1.        ]\n",
      "  [-1.         -1.         -1.         ... -0.98431373 -0.99215686\n",
      "   -1.        ]\n",
      "  [-1.         -1.         -1.         ... -1.         -1.\n",
      "   -1.        ]]\n",
      "\n",
      " [[-1.         -1.         -0.96862745 ... -0.05882353 -0.16862745\n",
      "   -0.3254902 ]\n",
      "  [-1.         -1.         -1.         ... -0.01960784 -0.00392157\n",
      "   -0.77254902]\n",
      "  [-1.         -0.92941176 -0.12941176 ... -0.01176471 -0.0745098\n",
      "   -0.52941176]\n",
      "  ...\n",
      "  [-1.         -0.75686275 -0.12941176 ... -0.97647059 -1.\n",
      "   -1.        ]\n",
      "  [-1.         -0.15294118 -0.00392157 ... -0.97647059 -1.\n",
      "   -1.        ]\n",
      "  [-1.         -0.27843137 -0.15294118 ... -1.         -1.\n",
      "   -1.        ]]\n",
      "\n",
      " [[-1.         -1.         -1.         ... -1.         -1.\n",
      "   -1.        ]\n",
      "  [-1.         -1.         -1.         ... -1.         -1.\n",
      "   -1.        ]\n",
      "  [-1.         -1.         -1.         ... -1.         -1.\n",
      "   -1.        ]\n",
      "  ...\n",
      "  [-0.89803922 -0.33333333 -0.96078431 ... -1.         -1.\n",
      "   -1.        ]\n",
      "  [-1.         -0.18431373 -0.3254902  ... -1.         -1.\n",
      "   -1.        ]\n",
      "  [-1.         -0.89019608 -0.89803922 ... -1.         -1.\n",
      "   -1.        ]]\n",
      "\n",
      " [[-0.96862745 -0.09803922 -0.00392157 ... -0.00392157 -0.01176471\n",
      "   -0.12156863]\n",
      "  [-0.96862745 -0.09803922 -0.00392157 ... -0.00392157 -0.01176471\n",
      "   -0.12156863]\n",
      "  [-0.96862745 -0.09803922 -0.00392157 ... -0.00392157 -0.01176471\n",
      "   -0.12156863]\n",
      "  ...\n",
      "  [-0.96862745 -0.09803922 -0.00392157 ... -0.00392157 -0.01176471\n",
      "   -0.12156863]\n",
      "  [-0.96862745 -0.09803922 -0.00392157 ... -0.00392157 -0.01176471\n",
      "   -0.12156863]\n",
      "  [-0.96862745 -0.09803922 -0.00392157 ... -0.00392157 -0.01176471\n",
      "   -0.12156863]]\n",
      "\n",
      " [[-0.44313725 -0.31764706 -0.19215686 ... -0.29411765 -0.67058824\n",
      "   -0.05882353]\n",
      "  [-0.9372549  -0.92156863 -0.89019608 ... -0.94509804 -0.95294118\n",
      "   -0.96862745]\n",
      "  [-1.         -1.         -1.         ... -1.         -1.\n",
      "   -1.        ]\n",
      "  ...\n",
      "  [-1.         -1.         -1.         ... -1.         -1.\n",
      "   -1.        ]\n",
      "  [-0.96078431 -0.94509804 -0.9372549  ... -0.94509804 -0.96078431\n",
      "   -0.97647059]\n",
      "  [-0.48235294 -0.38039216 -0.27843137 ... -0.30196078 -0.67058824\n",
      "   -0.05882353]]]\n",
      "[0 0 0 0 0]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "splitting...\n",
      "reshaping...\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = get_split_data(LARGE_DATASET_DIR, override=False, verbose=True)\n",
    "print('reshaping...')\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$28x28 \\rightarrow (conv: f=3, s=1, c=6) \\rightarrow 26x26x6 \\rightarrow (conv: f=4, s=2, c=8) \\rightarrow 11x11x8 \\rightarrow flatten \\rightarrow 968 \\rightarrow FC \\rightarrow 120 \\rightarrow softmax$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1\n",
      "Epochs: 10\n",
      "\n",
      "Epoch 1/10\n",
      "370379/370379 [==============================] - 142s 384us/step - loss: 0.4329 - acc: 0.8696\n",
      "Epoch 2/10\n",
      "370379/370379 [==============================] - 149s 402us/step - loss: 0.3275 - acc: 0.9006\n",
      "Epoch 3/10\n",
      "370379/370379 [==============================] - 138s 374us/step - loss: 0.2936 - acc: 0.9103\n",
      "Epoch 4/10\n",
      "370379/370379 [==============================] - 136s 367us/step - loss: 0.2721 - acc: 0.9165\n",
      "Epoch 5/10\n",
      "370379/370379 [==============================] - 150s 405us/step - loss: 0.2551 - acc: 0.9210\n",
      "Epoch 6/10\n",
      "370379/370379 [==============================] - 149s 402us/step - loss: 0.2391 - acc: 0.9257\n",
      "Epoch 7/10\n",
      "370379/370379 [==============================] - 146s 394us/step - loss: 0.2269 - acc: 0.9291\n",
      "Epoch 8/10\n",
      "370379/370379 [==============================] - 142s 384us/step - loss: 0.2162 - acc: 0.9321\n",
      "Epoch 9/10\n",
      "370379/370379 [==============================] - 138s 371us/step - loss: 0.2056 - acc: 0.9352\n",
      "Epoch 10/10\n",
      "370379/370379 [==============================] - 137s 369us/step - loss: 0.1976 - acc: 0.9373\n",
      "158735/158735 [==============================] - 20s 124us/step\n",
      "Loss: 0.35580147659586714\n",
      "Accuracy: 0.9068762402750471\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 10\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(6, kernel_size=(3, 3), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(8, kernel_size=(4, 4), strides=(2, 2), activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(120, activation='relu'),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(lr), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertical:\n",
    "$$28x28 \\rightarrow (conv: f=(5, 3), s=1, c=6) \\rightarrow 24x26x6 \\rightarrow (conv: f=4, s=2, c=8) \\rightarrow 11x12x8 \\rightarrow flatten \\rightarrow 1056 \\rightarrow FC \\rightarrow 256 \\rightarrow FC \\rightarrow 64 \\rightarrow softmax$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new net...\n",
      "Learning rate: 0.1\n",
      "Epochs: 10\n",
      "\n",
      "Epoch 1/10\n",
      "370379/370379 [==============================] - 161s 435us/step - loss: 0.4502 - acc: 0.8607\n",
      "Epoch 2/10\n",
      "370379/370379 [==============================] - 155s 419us/step - loss: 0.3246 - acc: 0.8989\n",
      "Epoch 3/10\n",
      "370379/370379 [==============================] - 164s 442us/step - loss: 0.2868 - acc: 0.9102\n",
      "Epoch 4/10\n",
      "370379/370379 [==============================] - 164s 442us/step - loss: 0.2598 - acc: 0.9181\n",
      "Epoch 5/10\n",
      "370379/370379 [==============================] - 162s 438us/step - loss: 0.2402 - acc: 0.9234\n",
      "Epoch 6/10\n",
      "370379/370379 [==============================] - 167s 450us/step - loss: 0.2241 - acc: 0.9282\n",
      "Epoch 7/10\n",
      "370379/370379 [==============================] - 155s 419us/step - loss: 0.2091 - acc: 0.9328\n",
      "Epoch 8/10\n",
      "370379/370379 [==============================] - 153s 412us/step - loss: 0.1984 - acc: 0.9355\n",
      "Epoch 9/10\n",
      "370379/370379 [==============================] - 146s 395us/step - loss: 0.1887 - acc: 0.9384\n",
      "Epoch 10/10\n",
      "370379/370379 [==============================] - 172s 464us/step - loss: 0.1787 - acc: 0.9417\n",
      "158735/158735 [==============================] - 24s 152us/step\n",
      "Loss: 0.3289851732716596\n",
      "Accuracy: 0.9126972627334866\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 10\n",
    "print('Building new net...')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(6, kernel_size=(5, 3), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(8, kernel_size=(4, 4), strides=(2, 2), activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(lr), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 + vertical:\n",
    "$$\n",
    "28x28 \\rightarrow (conv: f=(5, 3), s=1, c=6, tanh) \\rightarrow 24x26x6 \\rightarrow (maxpool: f=2, s=2) \\rightarrow 12x13x6 \\rightarrow (conv: f=3, s=1, c=8, tanh) \\rightarrow 10x11x8 \\rightarrow (maxpool: f=2, s=2) \\rightarrow 5x5x8 \\rightarrow flatten \\rightarrow 200 \\rightarrow FC(relu) \\rightarrow 120 \\rightarrow FC(relu) \\rightarrow 84 \\rightarrow softmax\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new net...\n",
      "Learning rate: 0.1\n",
      "Epochs: 10\n",
      "\n",
      "Epoch 1/10\n",
      "370379/370379 [==============================] - 220s 594us/step - loss: 0.4531 - acc: 0.8614\n",
      "Epoch 2/10\n",
      "370379/370379 [==============================] - 224s 605us/step - loss: 0.3525 - acc: 0.8909\n",
      "Epoch 3/10\n",
      "370379/370379 [==============================] - 223s 602us/step - loss: 0.3263 - acc: 0.8984\n",
      "Epoch 4/10\n",
      "370379/370379 [==============================] - 216s 583us/step - loss: 0.3111 - acc: 0.9027\n",
      "Epoch 5/10\n",
      "370379/370379 [==============================] - 212s 571us/step - loss: 0.3000 - acc: 0.9060\n",
      "Epoch 6/10\n",
      "370379/370379 [==============================] - 218s 587us/step - loss: 0.2922 - acc: 0.9085\n",
      "Epoch 7/10\n",
      "370379/370379 [==============================] - 216s 582us/step - loss: 0.2860 - acc: 0.9100\n",
      "Epoch 8/10\n",
      "370379/370379 [==============================] - 204s 550us/step - loss: 0.2812 - acc: 0.9114\n",
      "Epoch 9/10\n",
      "370379/370379 [==============================] - 199s 537us/step - loss: 0.2767 - acc: 0.9128\n",
      "Epoch 10/10\n",
      "370379/370379 [==============================] - 201s 542us/step - loss: 0.2731 - acc: 0.9137\n",
      "158735/158735 [==============================] - 42s 267us/step\n",
      "Loss: 0.3103148417033441\n",
      "Accuracy: 0.904784704066526\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 10\n",
    "print('Building new net...')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(6, kernel_size=(5, 3), strides=(1, 1), activation='tanh', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    keras.layers.Conv2D(8, kernel_size=(3, 3), strides=(1, 1), activation='tanh'),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(120, activation='relu'),\n",
    "    keras.layers.Dense(84, activation='relu'),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(lr), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom with maxpool:\n",
    "$$28x28 \\rightarrow (conv: f=3, s=1, c=6) \\rightarrow 26x26x6 \\rightarrow (maxpool: f=2, s=2) \\rightarrow 13x13x6 \\rightarrow (conv: f=3, s=2, c=16) \\rightarrow 5x5x16 \\rightarrow (maxpool: f=2, s=1) \\rightarrow 4x4x16 \\rightarrow flatten \\rightarrow 258 \\rightarrow FC \\rightarrow 120 \\rightarrow softmax$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new net...\n",
      "Learning rate: 0.1\n",
      "Epochs: 10\n",
      "\n",
      "Epoch 1/10\n",
      "370379/370379 [==============================] - 145s 390us/step - loss: 0.4383 - acc: 0.8676\n",
      "Epoch 2/10\n",
      "370379/370379 [==============================] - 143s 387us/step - loss: 0.3475 - acc: 0.8944\n",
      "Epoch 3/10\n",
      "370379/370379 [==============================] - 143s 386us/step - loss: 0.3229 - acc: 0.9013\n",
      "Epoch 4/10\n",
      "370379/370379 [==============================] - 143s 386us/step - loss: 0.3081 - acc: 0.9052\n",
      "Epoch 5/10\n",
      "370379/370379 [==============================] - 144s 389us/step - loss: 0.2987 - acc: 0.9081\n",
      "Epoch 6/10\n",
      "370379/370379 [==============================] - 144s 388us/step - loss: 0.2914 - acc: 0.9106\n",
      "Epoch 7/10\n",
      "370379/370379 [==============================] - 149s 402us/step - loss: 0.2848 - acc: 0.9121\n",
      "Epoch 8/10\n",
      "370379/370379 [==============================] - 146s 395us/step - loss: 0.2795 - acc: 0.9139\n",
      "Epoch 9/10\n",
      "370379/370379 [==============================] - 144s 390us/step - loss: 0.2748 - acc: 0.9151\n",
      "Epoch 10/10\n",
      "370379/370379 [==============================] - 144s 390us/step - loss: 0.2706 - acc: 0.9163\n",
      "158735/158735 [==============================] - 23s 146us/step\n",
      "Loss: 0.310997528786073\n",
      "Accuracy: 0.9060509654459294\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 10\n",
    "print('Building new net...')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(6, kernel_size=(3, 3), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    keras.layers.Conv2D(16, kernel_size=(3, 3), strides=(2, 2), activation='relu'),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(120, activation='relu'),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(lr), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5:\n",
    "$$\n",
    "28x28 \\rightarrow (conv: f=5, s=1, c=6, tanh) \\rightarrow 24x24x6 \\rightarrow (maxpool: f=2, s=2) \\rightarrow 12x12x6 \\rightarrow (conv: f=5, s=1, c=16) \\rightarrow 8x8x16 \\rightarrow (maxpool: f=2, s=2) \\rightarrow 4x4x16 \\rightarrow flatten \\rightarrow 256 \\rightarrow FC \\rightarrow 128 \\rightarrow FC \\rightarrow 64 \\rightarrow softmax\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LeNet-5...\n",
      "Learning rate: 0.1\n",
      "Epochs: 10\n",
      "\n",
      "Epoch 1/10\n",
      "370379/370379 [==============================] - 223s 603us/step - loss: 0.4256 - acc: 0.8720\n",
      "Epoch 2/10\n",
      "370379/370379 [==============================] - 220s 593us/step - loss: 0.3434 - acc: 0.8944\n",
      "Epoch 3/10\n",
      "370379/370379 [==============================] - 220s 594us/step - loss: 0.3212 - acc: 0.9006\n",
      "Epoch 4/10\n",
      "370379/370379 [==============================] - 221s 597us/step - loss: 0.3077 - acc: 0.9050\n",
      "Epoch 5/10\n",
      "370379/370379 [==============================] - 220s 595us/step - loss: 0.2990 - acc: 0.9070\n",
      "Epoch 6/10\n",
      "370379/370379 [==============================] - 223s 603us/step - loss: 0.2907 - acc: 0.9097\n",
      "Epoch 7/10\n",
      "370379/370379 [==============================] - 231s 623us/step - loss: 0.2850 - acc: 0.9108\n",
      "Epoch 8/10\n",
      "370379/370379 [==============================] - 231s 623us/step - loss: 0.2805 - acc: 0.9126\n",
      "Epoch 9/10\n",
      "370379/370379 [==============================] - 239s 645us/step - loss: 0.2756 - acc: 0.9140\n",
      "Epoch 10/10\n",
      "370379/370379 [==============================] - 238s 644us/step - loss: 0.2733 - acc: 0.9140\n",
      "158735/158735 [==============================] - 44s 277us/step\n",
      "Loss: 0.3198329660997025\n",
      "Accuracy: 0.9018174945664157\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 10\n",
    "print('Building LeNet-5...')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    keras.layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh'),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(120, activation='tanh'),\n",
    "    keras.layers.Dense(84, activation='tanh'),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(lr), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
