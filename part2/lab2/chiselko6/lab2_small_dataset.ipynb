{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При чтении будем хранить данные в переменной `CACHE`, чтобы ускорить загрузку. В случае их обновления, достаточно вызвать `read(override=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_DATASET_DIR = '../../nmnist/notMNIST_small/'\n",
    "LARGE_DATASET_DIR = '../../nmnist/notMNIST_large/'\n",
    "CACHE = {}\n",
    "LABEL_MAP = {}\n",
    "INV_LABEL_MAP = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(data_dir, override=False):\n",
    "    f_v = 0\n",
    "    global CACHE\n",
    "    if not CACHE.get(data_dir, []) or override:\n",
    "        CACHE[data_dir] = []\n",
    "        X, y = [], []\n",
    "        for f in tqdm(os.listdir(data_dir), desc='Letter'):\n",
    "            if not f.startswith('.'):\n",
    "                img_dir = os.path.join(data_dir, f)\n",
    "                for img in os.listdir(img_dir):\n",
    "                    img_path = os.path.join(img_dir, img)\n",
    "                    data = cv2.imread(img_path, 0)\n",
    "                    if data is None:\n",
    "                        continue\n",
    "                    X.append(data * 2 / 255 - 1)\n",
    "                    if LABEL_MAP.get(f) is None:\n",
    "                        LABEL_MAP[f] = f_v\n",
    "                        INV_LABEL_MAP[f_v] = f\n",
    "                        f_v += 1\n",
    "                    y.append(LABEL_MAP[f])\n",
    "        CACHE[data_dir].append(np.array(X))\n",
    "        CACHE[data_dir].append(np.array(y))\n",
    "    return CACHE[data_dir][0], CACHE[data_dir][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir, verbose=False, override=False):\n",
    "    X, y = read(data_dir, override=override)\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    N = X.shape[0]\n",
    "    if verbose:\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        print(X[:5])\n",
    "        print(y[:5])\n",
    "        print(np.unique(y))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_data(data_dir, size=(0.7, 0.3), verbose=False, random_state=6, override=False):\n",
    "    X, y = shuffle(*get_data(data_dir, verbose=verbose, override=override), random_state=random_state)\n",
    "    assert abs(np.sum(size) - 1.0) < 0.001\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size[1], random_state=random_state)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcc5a8d7b6a4be1969cd06e415145aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Letter', max=11, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = get_split_data(SMALL_DATASET_DIR, override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим 5 слоев с функциями активации `ReLU` и `softmax` на последнем слое, коэффициент обучения $0.001$, $100$ эпох:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Building with new architecture...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512(relu) - 256(relu) - 64(relu) - 32(relu) - 10(softmax)\n",
      "Learning rate: 0.001\n",
      "Epochs: 100\n",
      "\n",
      "Epoch 1/100\n",
      "13106/13106 [==============================] - 6s 479us/step - loss: 2.1814 - acc: 0.2827\n",
      "Epoch 2/100\n",
      "13106/13106 [==============================] - 3s 211us/step - loss: 1.8574 - acc: 0.5022\n",
      "Epoch 3/100\n",
      "13106/13106 [==============================] - 3s 203us/step - loss: 1.4682 - acc: 0.6814\n",
      "Epoch 4/100\n",
      "13106/13106 [==============================] - 3s 207us/step - loss: 1.1076 - acc: 0.7703\n",
      "Epoch 5/100\n",
      "13106/13106 [==============================] - 3s 222us/step - loss: 0.8836 - acc: 0.8093\n",
      "Epoch 6/100\n",
      "13106/13106 [==============================] - 3s 204us/step - loss: 0.7539 - acc: 0.8269\n",
      "Epoch 7/100\n",
      "13106/13106 [==============================] - 3s 202us/step - loss: 0.6738 - acc: 0.8401\n",
      "Epoch 8/100\n",
      "13106/13106 [==============================] - 3s 215us/step - loss: 0.6218 - acc: 0.8486\n",
      "Epoch 9/100\n",
      "13106/13106 [==============================] - 3s 215us/step - loss: 0.5839 - acc: 0.8537\n",
      "Epoch 10/100\n",
      "13106/13106 [==============================] - 3s 204us/step - loss: 0.5568 - acc: 0.8591\n",
      "Epoch 11/100\n",
      "13106/13106 [==============================] - 3s 220us/step - loss: 0.5355 - acc: 0.8651\n",
      "Epoch 12/100\n",
      "13106/13106 [==============================] - 3s 206us/step - loss: 0.5184 - acc: 0.8691\n",
      "Epoch 13/100\n",
      "13106/13106 [==============================] - 3s 205us/step - loss: 0.5046 - acc: 0.8705\n",
      "Epoch 14/100\n",
      "13106/13106 [==============================] - 3s 202us/step - loss: 0.4921 - acc: 0.8740\n",
      "Epoch 15/100\n",
      "13106/13106 [==============================] - 3s 233us/step - loss: 0.4814 - acc: 0.8749\n",
      "Epoch 16/100\n",
      "13106/13106 [==============================] - 3s 217us/step - loss: 0.4715 - acc: 0.8793\n",
      "Epoch 17/100\n",
      "13106/13106 [==============================] - 3s 217us/step - loss: 0.4622 - acc: 0.8806\n",
      "Epoch 18/100\n",
      "13106/13106 [==============================] - 3s 217us/step - loss: 0.4560 - acc: 0.8821\n",
      "Epoch 19/100\n",
      "13106/13106 [==============================] - 3s 215us/step - loss: 0.4495 - acc: 0.8857\n",
      "Epoch 20/100\n",
      "13106/13106 [==============================] - 3s 216us/step - loss: 0.4426 - acc: 0.8867\n",
      "Epoch 21/100\n",
      "13106/13106 [==============================] - 3s 221us/step - loss: 0.4356 - acc: 0.8864\n",
      "Epoch 22/100\n",
      "13106/13106 [==============================] - 3s 242us/step - loss: 0.4316 - acc: 0.8883\n",
      "Epoch 23/100\n",
      "13106/13106 [==============================] - 4s 270us/step - loss: 0.4251 - acc: 0.8895\n",
      "Epoch 24/100\n",
      "13106/13106 [==============================] - 3s 240us/step - loss: 0.4201 - acc: 0.8909\n",
      "Epoch 25/100\n",
      "13106/13106 [==============================] - 3s 208us/step - loss: 0.4149 - acc: 0.8920\n",
      "Epoch 26/100\n",
      "13106/13106 [==============================] - 3s 209us/step - loss: 0.4097 - acc: 0.8929\n",
      "Epoch 27/100\n",
      "13106/13106 [==============================] - 3s 210us/step - loss: 0.4052 - acc: 0.8954\n",
      "Epoch 28/100\n",
      "13106/13106 [==============================] - 3s 263us/step - loss: 0.4013 - acc: 0.8948\n",
      "Epoch 29/100\n",
      "13106/13106 [==============================] - 4s 272us/step - loss: 0.3969 - acc: 0.8968\n",
      "Epoch 30/100\n",
      "13106/13106 [==============================] - 5s 358us/step - loss: 0.3922 - acc: 0.8962\n",
      "Epoch 31/100\n",
      "13106/13106 [==============================] - 6s 487us/step - loss: 0.3885 - acc: 0.8977\n",
      "Epoch 32/100\n",
      "13106/13106 [==============================] - 4s 339us/step - loss: 0.3844 - acc: 0.8985\n",
      "Epoch 33/100\n",
      "13106/13106 [==============================] - 5s 416us/step - loss: 0.3810 - acc: 0.8996\n",
      "Epoch 34/100\n",
      "13106/13106 [==============================] - 3s 244us/step - loss: 0.3763 - acc: 0.9010\n",
      "Epoch 35/100\n",
      "13106/13106 [==============================] - 3s 237us/step - loss: 0.3736 - acc: 0.9020\n",
      "Epoch 36/100\n",
      "13106/13106 [==============================] - 3s 220us/step - loss: 0.3691 - acc: 0.9030\n",
      "Epoch 37/100\n",
      "13106/13106 [==============================] - 3s 238us/step - loss: 0.3665 - acc: 0.9029\n",
      "Epoch 38/100\n",
      "13106/13106 [==============================] - 3s 263us/step - loss: 0.3633 - acc: 0.9036\n",
      "Epoch 39/100\n",
      "13106/13106 [==============================] - 3s 207us/step - loss: 0.3596 - acc: 0.9029\n",
      "Epoch 40/100\n",
      "13106/13106 [==============================] - 3s 211us/step - loss: 0.3565 - acc: 0.9058\n",
      "Epoch 41/100\n",
      "13106/13106 [==============================] - 3s 229us/step - loss: 0.3536 - acc: 0.9061\n",
      "Epoch 42/100\n",
      "13106/13106 [==============================] - 4s 306us/step - loss: 0.3498 - acc: 0.9063\n",
      "Epoch 43/100\n",
      "13106/13106 [==============================] - 3s 238us/step - loss: 0.3477 - acc: 0.9072\n",
      "Epoch 44/100\n",
      "13106/13106 [==============================] - 4s 271us/step - loss: 0.3444 - acc: 0.9086\n",
      "Epoch 45/100\n",
      "13106/13106 [==============================] - 3s 193us/step - loss: 0.3403 - acc: 0.9110\n",
      "Epoch 46/100\n",
      "13106/13106 [==============================] - 3s 197us/step - loss: 0.3385 - acc: 0.9093\n",
      "Epoch 47/100\n",
      "13106/13106 [==============================] - 2s 186us/step - loss: 0.3351 - acc: 0.9087\n",
      "Epoch 48/100\n",
      "13106/13106 [==============================] - 2s 189us/step - loss: 0.3322 - acc: 0.9123\n",
      "Epoch 49/100\n",
      "13106/13106 [==============================] - 3s 198us/step - loss: 0.3303 - acc: 0.9121\n",
      "Epoch 50/100\n",
      "13106/13106 [==============================] - 3s 215us/step - loss: 0.3266 - acc: 0.9126\n",
      "Epoch 51/100\n",
      "13106/13106 [==============================] - 2s 188us/step - loss: 0.3236 - acc: 0.9142\n",
      "Epoch 52/100\n",
      "13106/13106 [==============================] - 2s 181us/step - loss: 0.3217 - acc: 0.9139\n",
      "Epoch 53/100\n",
      "13106/13106 [==============================] - 3s 202us/step - loss: 0.3188 - acc: 0.9130\n",
      "Epoch 54/100\n",
      "13106/13106 [==============================] - 2s 184us/step - loss: 0.3152 - acc: 0.9152\n",
      "Epoch 55/100\n",
      "13106/13106 [==============================] - 2s 188us/step - loss: 0.3126 - acc: 0.9158\n",
      "Epoch 56/100\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.3103 - acc: 0.9161\n",
      "Epoch 57/100\n",
      "13106/13106 [==============================] - 2s 184us/step - loss: 0.3075 - acc: 0.9158\n",
      "Epoch 58/100\n",
      "13106/13106 [==============================] - 2s 188us/step - loss: 0.3055 - acc: 0.9169\n",
      "Epoch 59/100\n",
      "13106/13106 [==============================] - 2s 185us/step - loss: 0.3029 - acc: 0.9168\n",
      "Epoch 60/100\n",
      "13106/13106 [==============================] - 2s 186us/step - loss: 0.3002 - acc: 0.9186\n",
      "Epoch 61/100\n",
      "13106/13106 [==============================] - 2s 186us/step - loss: 0.2972 - acc: 0.9187\n",
      "Epoch 62/100\n",
      "13106/13106 [==============================] - 2s 186us/step - loss: 0.2951 - acc: 0.9197\n",
      "Epoch 63/100\n",
      "13106/13106 [==============================] - 3s 207us/step - loss: 0.2926 - acc: 0.9196\n",
      "Epoch 64/100\n",
      "13106/13106 [==============================] - 2s 187us/step - loss: 0.2907 - acc: 0.9203\n",
      "Epoch 65/100\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.2882 - acc: 0.9207\n",
      "Epoch 66/100\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.2858 - acc: 0.9221\n",
      "Epoch 67/100\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.2826 - acc: 0.9216\n",
      "Epoch 68/100\n",
      "13106/13106 [==============================] - 3s 200us/step - loss: 0.2809 - acc: 0.9233\n",
      "Epoch 69/100\n",
      "13106/13106 [==============================] - 3s 202us/step - loss: 0.2780 - acc: 0.9245\n",
      "Epoch 70/100\n",
      "13106/13106 [==============================] - 3s 198us/step - loss: 0.2763 - acc: 0.9241\n",
      "Epoch 71/100\n",
      "13106/13106 [==============================] - 3s 194us/step - loss: 0.2738 - acc: 0.9242\n",
      "Epoch 72/100\n",
      "13106/13106 [==============================] - 3s 191us/step - loss: 0.2716 - acc: 0.9255\n",
      "Epoch 73/100\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.2692 - acc: 0.9266\n",
      "Epoch 74/100\n",
      "13106/13106 [==============================] - 2s 190us/step - loss: 0.2672 - acc: 0.9251\n",
      "Epoch 75/100\n",
      "13106/13106 [==============================] - 2s 187us/step - loss: 0.2654 - acc: 0.9271\n",
      "Epoch 76/100\n",
      "13106/13106 [==============================] - 2s 181us/step - loss: 0.2624 - acc: 0.9283\n",
      "Epoch 77/100\n",
      "13106/13106 [==============================] - 3s 196us/step - loss: 0.2611 - acc: 0.9274\n",
      "Epoch 78/100\n",
      "13106/13106 [==============================] - 2s 185us/step - loss: 0.2590 - acc: 0.9290\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13106/13106 [==============================] - 2s 187us/step - loss: 0.2563 - acc: 0.9301\n",
      "Epoch 80/100\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.2542 - acc: 0.9312\n",
      "Epoch 81/100\n",
      "13106/13106 [==============================] - 2s 189us/step - loss: 0.2520 - acc: 0.9299\n",
      "Epoch 82/100\n",
      "13106/13106 [==============================] - 2s 186us/step - loss: 0.2494 - acc: 0.9314\n",
      "Epoch 83/100\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.2481 - acc: 0.9302\n",
      "Epoch 84/100\n",
      "13106/13106 [==============================] - 3s 201us/step - loss: 0.2456 - acc: 0.9320\n",
      "Epoch 85/100\n",
      "13106/13106 [==============================] - 2s 188us/step - loss: 0.2441 - acc: 0.9321\n",
      "Epoch 86/100\n",
      "13106/13106 [==============================] - 2s 187us/step - loss: 0.2418 - acc: 0.9328\n",
      "Epoch 87/100\n",
      "13106/13106 [==============================] - 2s 188us/step - loss: 0.2402 - acc: 0.9334\n",
      "Epoch 88/100\n",
      "13106/13106 [==============================] - 2s 187us/step - loss: 0.2375 - acc: 0.9342\n",
      "Epoch 89/100\n",
      "13106/13106 [==============================] - 2s 190us/step - loss: 0.2365 - acc: 0.9342\n",
      "Epoch 90/100\n",
      "13106/13106 [==============================] - 3s 210us/step - loss: 0.2330 - acc: 0.9355\n",
      "Epoch 91/100\n",
      "13106/13106 [==============================] - 2s 185us/step - loss: 0.2318 - acc: 0.9345\n",
      "Epoch 92/100\n",
      "13106/13106 [==============================] - 2s 189us/step - loss: 0.2297 - acc: 0.9367\n",
      "Epoch 93/100\n",
      "13106/13106 [==============================] - 3s 208us/step - loss: 0.2280 - acc: 0.9369\n",
      "Epoch 94/100\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.2256 - acc: 0.9377\n",
      "Epoch 95/100\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.2234 - acc: 0.9368\n",
      "Epoch 96/100\n",
      "13106/13106 [==============================] - 2s 190us/step - loss: 0.2217 - acc: 0.9367\n",
      "Epoch 97/100\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.2192 - acc: 0.9386\n",
      "Epoch 98/100\n",
      "13106/13106 [==============================] - 2s 184us/step - loss: 0.2180 - acc: 0.9390\n",
      "Epoch 99/100\n",
      "13106/13106 [==============================] - 2s 186us/step - loss: 0.2156 - acc: 0.9396\n",
      "Epoch 100/100\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.2141 - acc: 0.9396\n",
      "5618/5618 [==============================] - 2s 350us/step\n",
      "Loss: 0.35672372748001735\n",
      "Accuracy: 0.8971164115343538\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "epochs = 100\n",
    "printmd('**Building with new architecture...**')\n",
    "print('512(relu) - 256(relu) - 64(relu) - 32(relu) - 10(softmax)')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(lr), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменим коэффициент обучения на больший `lr = 0.1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Building with new architecture...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512(relu) - 256(relu) - 64(relu) - 32(relu) - 10(softmax)\n",
      "Learning rate: 0.1\n",
      "Epochs: 50\n",
      "\n",
      "Epoch 1/50\n",
      "13106/13106 [==============================] - 7s 500us/step - loss: 0.8130 - acc: 0.7437\n",
      "Epoch 2/50\n",
      "13106/13106 [==============================] - 3s 197us/step - loss: 0.4181 - acc: 0.8751\n",
      "Epoch 3/50\n",
      "13106/13106 [==============================] - 3s 220us/step - loss: 0.3578 - acc: 0.8879\n",
      "Epoch 4/50\n",
      "13106/13106 [==============================] - 3s 249us/step - loss: 0.3233 - acc: 0.9003\n",
      "Epoch 5/50\n",
      "13106/13106 [==============================] - 3s 232us/step - loss: 0.2836 - acc: 0.9113\n",
      "Epoch 6/50\n",
      "13106/13106 [==============================] - 3s 262us/step - loss: 0.2604 - acc: 0.9177\n",
      "Epoch 7/50\n",
      "13106/13106 [==============================] - 4s 303us/step - loss: 0.2366 - acc: 0.9259\n",
      "Epoch 8/50\n",
      "13106/13106 [==============================] - 3s 210us/step - loss: 0.2303 - acc: 0.9263\n",
      "Epoch 9/50\n",
      "13106/13106 [==============================] - 3s 213us/step - loss: 0.2094 - acc: 0.9295\n",
      "Epoch 10/50\n",
      "13106/13106 [==============================] - 3s 218us/step - loss: 0.1972 - acc: 0.9352\n",
      "Epoch 11/50\n",
      "13106/13106 [==============================] - 3s 240us/step - loss: 0.1773 - acc: 0.9401\n",
      "Epoch 12/50\n",
      "13106/13106 [==============================] - 3s 244us/step - loss: 0.1631 - acc: 0.9455\n",
      "Epoch 13/50\n",
      "13106/13106 [==============================] - 3s 233us/step - loss: 0.1615 - acc: 0.9477\n",
      "Epoch 14/50\n",
      "13106/13106 [==============================] - 3s 219us/step - loss: 0.1452 - acc: 0.9535\n",
      "Epoch 15/50\n",
      "13106/13106 [==============================] - 3s 217us/step - loss: 0.1359 - acc: 0.9538\n",
      "Epoch 16/50\n",
      "13106/13106 [==============================] - 5s 376us/step - loss: 0.1297 - acc: 0.9581\n",
      "Epoch 17/50\n",
      "13106/13106 [==============================] - 3s 234us/step - loss: 0.1262 - acc: 0.9584\n",
      "Epoch 18/50\n",
      "13106/13106 [==============================] - 4s 304us/step - loss: 0.1252 - acc: 0.9592\n",
      "Epoch 19/50\n",
      "13106/13106 [==============================] - 3s 225us/step - loss: 0.1104 - acc: 0.9626\n",
      "Epoch 20/50\n",
      "13106/13106 [==============================] - 3s 229us/step - loss: 0.1123 - acc: 0.9651\n",
      "Epoch 21/50\n",
      "13106/13106 [==============================] - 3s 226us/step - loss: 0.1013 - acc: 0.9676\n",
      "Epoch 22/50\n",
      "13106/13106 [==============================] - 3s 217us/step - loss: 0.0885 - acc: 0.9714\n",
      "Epoch 23/50\n",
      "13106/13106 [==============================] - 3s 217us/step - loss: 0.0898 - acc: 0.9709\n",
      "Epoch 24/50\n",
      "13106/13106 [==============================] - 3s 208us/step - loss: 0.0962 - acc: 0.9686\n",
      "Epoch 25/50\n",
      "13106/13106 [==============================] - 3s 234us/step - loss: 0.0740 - acc: 0.9766\n",
      "Epoch 26/50\n",
      "13106/13106 [==============================] - 3s 198us/step - loss: 0.0881 - acc: 0.9719\n",
      "Epoch 27/50\n",
      "13106/13106 [==============================] - 3s 197us/step - loss: 0.0670 - acc: 0.9788\n",
      "Epoch 28/50\n",
      "13106/13106 [==============================] - 3s 200us/step - loss: 0.0813 - acc: 0.9746\n",
      "Epoch 29/50\n",
      "13106/13106 [==============================] - 3s 223us/step - loss: 0.0659 - acc: 0.9800\n",
      "Epoch 30/50\n",
      "13106/13106 [==============================] - 3s 216us/step - loss: 0.0529 - acc: 0.9836\n",
      "Epoch 31/50\n",
      "13106/13106 [==============================] - 3s 214us/step - loss: 0.0585 - acc: 0.9821\n",
      "Epoch 32/50\n",
      "13106/13106 [==============================] - 3s 260us/step - loss: 0.0742 - acc: 0.9770\n",
      "Epoch 33/50\n",
      "13106/13106 [==============================] - 3s 229us/step - loss: 0.0409 - acc: 0.9866\n",
      "Epoch 34/50\n",
      "13106/13106 [==============================] - 3s 262us/step - loss: 0.0623 - acc: 0.9823\n",
      "Epoch 35/50\n",
      "13106/13106 [==============================] - 2s 175us/step - loss: 0.0708 - acc: 0.9771\n",
      "Epoch 36/50\n",
      "13106/13106 [==============================] - 2s 176us/step - loss: 0.0515 - acc: 0.9831\n",
      "Epoch 37/50\n",
      "13106/13106 [==============================] - 2s 180us/step - loss: 0.0519 - acc: 0.9826\n",
      "Epoch 38/50\n",
      "13106/13106 [==============================] - 2s 175us/step - loss: 0.0493 - acc: 0.9840\n",
      "Epoch 39/50\n",
      "13106/13106 [==============================] - 2s 175us/step - loss: 0.0299 - acc: 0.9913\n",
      "Epoch 40/50\n",
      "13106/13106 [==============================] - 2s 177us/step - loss: 0.0441 - acc: 0.9867\n",
      "Epoch 41/50\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.0498 - acc: 0.9841\n",
      "Epoch 42/50\n",
      "13106/13106 [==============================] - 2s 179us/step - loss: 0.0291 - acc: 0.9907\n",
      "Epoch 43/50\n",
      "13106/13106 [==============================] - 2s 178us/step - loss: 0.0533 - acc: 0.9828\n",
      "Epoch 44/50\n",
      "13106/13106 [==============================] - 2s 178us/step - loss: 0.0387 - acc: 0.9876\n",
      "Epoch 45/50\n",
      "13106/13106 [==============================] - 2s 188us/step - loss: 0.0504 - acc: 0.9862\n",
      "Epoch 46/50\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.0428 - acc: 0.9880\n",
      "Epoch 47/50\n",
      "13106/13106 [==============================] - 2s 179us/step - loss: 0.0376 - acc: 0.9891\n",
      "Epoch 48/50\n",
      "13106/13106 [==============================] - 2s 181us/step - loss: 0.0181 - acc: 0.9946\n",
      "Epoch 49/50\n",
      "13106/13106 [==============================] - 2s 186us/step - loss: 0.0181 - acc: 0.9944\n",
      "Epoch 50/50\n",
      "13106/13106 [==============================] - 2s 178us/step - loss: 0.0336 - acc: 0.9899\n",
      "5618/5618 [==============================] - 2s 348us/step\n",
      "Loss: 0.503723808802721\n",
      "Accuracy: 0.9186543253826984\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 50\n",
    "printmd('**Building with new architecture...**')\n",
    "print('512(relu) - 256(relu) - 64(relu) - 32(relu) - 10(softmax)')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(lr), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность увеличилась, попробуем найти оптимальный коэффициент обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Building with new architecture...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512(relu) - 256(relu) - 64(relu) - 32(relu) - 10(softmax)\n",
      "Learning rate: 0.2\n",
      "Epochs: 50\n",
      "\n",
      "Epoch 1/50\n",
      "13106/13106 [==============================] - 5s 374us/step - loss: 1.0957 - acc: 0.6366\n",
      "Epoch 2/50\n",
      "13106/13106 [==============================] - 4s 273us/step - loss: 0.5060 - acc: 0.8438\n",
      "Epoch 3/50\n",
      "13106/13106 [==============================] - 4s 288us/step - loss: 0.3948 - acc: 0.8808\n",
      "Epoch 4/50\n",
      "13106/13106 [==============================] - 2s 188us/step - loss: 0.3563 - acc: 0.8901\n",
      "Epoch 5/50\n",
      "13106/13106 [==============================] - 2s 185us/step - loss: 0.3343 - acc: 0.8956\n",
      "Epoch 6/50\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.3060 - acc: 0.9049\n",
      "Epoch 7/50\n",
      "13106/13106 [==============================] - 3s 234us/step - loss: 0.2793 - acc: 0.9126\n",
      "Epoch 8/50\n",
      "13106/13106 [==============================] - 3s 194us/step - loss: 0.2602 - acc: 0.9161\n",
      "Epoch 9/50\n",
      "13106/13106 [==============================] - 3s 205us/step - loss: 0.2531 - acc: 0.9191\n",
      "Epoch 10/50\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.2340 - acc: 0.9248\n",
      "Epoch 11/50\n",
      "13106/13106 [==============================] - 2s 181us/step - loss: 0.2432 - acc: 0.9215\n",
      "Epoch 12/50\n",
      "13106/13106 [==============================] - 2s 187us/step - loss: 0.2229 - acc: 0.9273\n",
      "Epoch 13/50\n",
      "13106/13106 [==============================] - 3s 240us/step - loss: 0.2071 - acc: 0.9341\n",
      "Epoch 14/50\n",
      "13106/13106 [==============================] - 3s 200us/step - loss: 0.1919 - acc: 0.9358\n",
      "Epoch 15/50\n",
      "13106/13106 [==============================] - 2s 185us/step - loss: 0.1959 - acc: 0.9384\n",
      "Epoch 16/50\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.1818 - acc: 0.9406\n",
      "Epoch 17/50\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.1653 - acc: 0.9435\n",
      "Epoch 18/50\n",
      "13106/13106 [==============================] - 3s 198us/step - loss: 0.1681 - acc: 0.9437\n",
      "Epoch 19/50\n",
      "13106/13106 [==============================] - 3s 232us/step - loss: 0.1607 - acc: 0.9477\n",
      "Epoch 20/50\n",
      "13106/13106 [==============================] - 3s 218us/step - loss: 0.1543 - acc: 0.9506\n",
      "Epoch 21/50\n",
      "13106/13106 [==============================] - 2s 190us/step - loss: 0.1424 - acc: 0.9530\n",
      "Epoch 22/50\n",
      "13106/13106 [==============================] - 2s 185us/step - loss: 0.1350 - acc: 0.9557\n",
      "Epoch 23/50\n",
      "13106/13106 [==============================] - 3s 197us/step - loss: 0.1264 - acc: 0.9597\n",
      "Epoch 24/50\n",
      "13106/13106 [==============================] - 3s 238us/step - loss: 0.1464 - acc: 0.9541\n",
      "Epoch 25/50\n",
      "13106/13106 [==============================] - 2s 185us/step - loss: 0.1469 - acc: 0.9543\n",
      "Epoch 26/50\n",
      "13106/13106 [==============================] - 3s 194us/step - loss: 0.1191 - acc: 0.9605\n",
      "Epoch 27/50\n",
      "13106/13106 [==============================] - 2s 176us/step - loss: 0.1371 - acc: 0.9557\n",
      "Epoch 28/50\n",
      "13106/13106 [==============================] - 2s 180us/step - loss: 0.1209 - acc: 0.9602\n",
      "Epoch 29/50\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.1053 - acc: 0.9651\n",
      "Epoch 30/50\n",
      "13106/13106 [==============================] - 2s 187us/step - loss: 0.1009 - acc: 0.9668\n",
      "Epoch 31/50\n",
      "13106/13106 [==============================] - 3s 203us/step - loss: 0.1045 - acc: 0.9678\n",
      "Epoch 32/50\n",
      "13106/13106 [==============================] - 3s 221us/step - loss: 0.0990 - acc: 0.9685\n",
      "Epoch 33/50\n",
      "13106/13106 [==============================] - 3s 191us/step - loss: 0.1011 - acc: 0.9667\n",
      "Epoch 34/50\n",
      "13106/13106 [==============================] - 3s 208us/step - loss: 0.0897 - acc: 0.9701\n",
      "Epoch 35/50\n",
      "13106/13106 [==============================] - 3s 215us/step - loss: 0.0800 - acc: 0.9740\n",
      "Epoch 36/50\n",
      "13106/13106 [==============================] - 3s 191us/step - loss: 0.0956 - acc: 0.9710\n",
      "Epoch 37/50\n",
      "13106/13106 [==============================] - 2s 185us/step - loss: 0.0904 - acc: 0.9711\n",
      "Epoch 38/50\n",
      "13106/13106 [==============================] - 3s 198us/step - loss: 0.0727 - acc: 0.9765\n",
      "Epoch 39/50\n",
      "13106/13106 [==============================] - 3s 205us/step - loss: 0.0901 - acc: 0.9730\n",
      "Epoch 40/50\n",
      "13106/13106 [==============================] - 2s 175us/step - loss: 0.0666 - acc: 0.9772\n",
      "Epoch 41/50\n",
      "13106/13106 [==============================] - 2s 174us/step - loss: 0.0815 - acc: 0.9742\n",
      "Epoch 42/50\n",
      "13106/13106 [==============================] - 2s 174us/step - loss: 0.0678 - acc: 0.9776\n",
      "Epoch 43/50\n",
      "13106/13106 [==============================] - 2s 177us/step - loss: 0.0705 - acc: 0.9781\n",
      "Epoch 44/50\n",
      "13106/13106 [==============================] - 2s 178us/step - loss: 0.0761 - acc: 0.9762\n",
      "Epoch 45/50\n",
      "13106/13106 [==============================] - 2s 175us/step - loss: 0.0851 - acc: 0.9752\n",
      "Epoch 46/50\n",
      "13106/13106 [==============================] - 2s 174us/step - loss: 0.0674 - acc: 0.9799\n",
      "Epoch 47/50\n",
      "13106/13106 [==============================] - 2s 177us/step - loss: 0.0897 - acc: 0.9738\n",
      "Epoch 48/50\n",
      "13106/13106 [==============================] - 2s 180us/step - loss: 0.0619 - acc: 0.9803\n",
      "Epoch 49/50\n",
      "13106/13106 [==============================] - 3s 192us/step - loss: 0.0608 - acc: 0.9808\n",
      "Epoch 50/50\n",
      "13106/13106 [==============================] - 2s 176us/step - loss: 0.0724 - acc: 0.9772\n",
      "5618/5618 [==============================] - 1s 234us/step\n",
      "Loss: 0.466606018412621\n",
      "Accuracy: 0.9120683517265931\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.2\n",
    "epochs = 50\n",
    "printmd('**Building with new architecture...**')\n",
    "print('512(relu) - 256(relu) - 64(relu) - 32(relu) - 10(softmax)')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(lr), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, точность слегка упала. Поэтому экспериментально будем считать, что коэффициент обучения, увеличение которого ухудшает точность, равен `lr_opt = 0.1`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее попробуем применить другие функции активации, например `tanh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Building with new architecture...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512(tanh) - 128(tanh) - 32(tanh) - 10(softmax)\n",
      "Learning rate: 0.1\n",
      "Epochs: 50\n",
      "\n",
      "Epoch 1/50\n",
      "13106/13106 [==============================] - 5s 394us/step - loss: 0.6870 - acc: 0.7992\n",
      "Epoch 2/50\n",
      "13106/13106 [==============================] - 3s 208us/step - loss: 0.4098 - acc: 0.8787\n",
      "Epoch 3/50\n",
      "13106/13106 [==============================] - 3s 201us/step - loss: 0.3511 - acc: 0.8943\n",
      "Epoch 4/50\n",
      "13106/13106 [==============================] - 3s 219us/step - loss: 0.3126 - acc: 0.9070\n",
      "Epoch 5/50\n",
      "13106/13106 [==============================] - 3s 226us/step - loss: 0.2878 - acc: 0.9116\n",
      "Epoch 6/50\n",
      "13106/13106 [==============================] - 3s 259us/step - loss: 0.2575 - acc: 0.9213\n",
      "Epoch 7/50\n",
      "13106/13106 [==============================] - 3s 219us/step - loss: 0.2393 - acc: 0.9257\n",
      "Epoch 8/50\n",
      "13106/13106 [==============================] - 3s 220us/step - loss: 0.2225 - acc: 0.9298\n",
      "Epoch 9/50\n",
      "13106/13106 [==============================] - 3s 217us/step - loss: 0.1981 - acc: 0.9384\n",
      "Epoch 10/50\n",
      "13106/13106 [==============================] - 3s 222us/step - loss: 0.1811 - acc: 0.9435\n",
      "Epoch 11/50\n",
      "13106/13106 [==============================] - 4s 315us/step - loss: 0.1651 - acc: 0.9473\n",
      "Epoch 12/50\n",
      "13106/13106 [==============================] - 3s 262us/step - loss: 0.1501 - acc: 0.9522\n",
      "Epoch 13/50\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.1383 - acc: 0.9565\n",
      "Epoch 14/50\n",
      "13106/13106 [==============================] - 2s 179us/step - loss: 0.1254 - acc: 0.9602\n",
      "Epoch 15/50\n",
      "13106/13106 [==============================] - 2s 181us/step - loss: 0.1140 - acc: 0.9655\n",
      "Epoch 16/50\n",
      "13106/13106 [==============================] - 2s 180us/step - loss: 0.1080 - acc: 0.9668\n",
      "Epoch 17/50\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.0937 - acc: 0.9717\n",
      "Epoch 18/50\n",
      "13106/13106 [==============================] - 3s 198us/step - loss: 0.0926 - acc: 0.9720\n",
      "Epoch 19/50\n",
      "13106/13106 [==============================] - 3s 194us/step - loss: 0.0799 - acc: 0.9759\n",
      "Epoch 20/50\n",
      "13106/13106 [==============================] - 2s 181us/step - loss: 0.0833 - acc: 0.9754\n",
      "Epoch 21/50\n",
      "13106/13106 [==============================] - 3s 197us/step - loss: 0.0696 - acc: 0.9792\n",
      "Epoch 22/50\n",
      "13106/13106 [==============================] - 3s 194us/step - loss: 0.0587 - acc: 0.9827\n",
      "Epoch 23/50\n",
      "13106/13106 [==============================] - 2s 190us/step - loss: 0.0615 - acc: 0.9815\n",
      "Epoch 24/50\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.0460 - acc: 0.9868\n",
      "Epoch 25/50\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.0456 - acc: 0.9873\n",
      "Epoch 26/50\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.0400 - acc: 0.9893\n",
      "Epoch 27/50\n",
      "13106/13106 [==============================] - 2s 186us/step - loss: 0.0384 - acc: 0.9895\n",
      "Epoch 28/50\n",
      "13106/13106 [==============================] - 2s 182us/step - loss: 0.0531 - acc: 0.9844\n",
      "Epoch 29/50\n",
      "13106/13106 [==============================] - 2s 181us/step - loss: 0.0435 - acc: 0.9871\n",
      "Epoch 30/50\n",
      "13106/13106 [==============================] - 2s 190us/step - loss: 0.0682 - acc: 0.9786\n",
      "Epoch 31/50\n",
      "13106/13106 [==============================] - 3s 204us/step - loss: 0.0401 - acc: 0.9895\n",
      "Epoch 32/50\n",
      "13106/13106 [==============================] - 3s 235us/step - loss: 0.0444 - acc: 0.9873\n",
      "Epoch 33/50\n",
      "13106/13106 [==============================] - 4s 294us/step - loss: 0.0206 - acc: 0.9950\n",
      "Epoch 34/50\n",
      "13106/13106 [==============================] - 3s 226us/step - loss: 0.0141 - acc: 0.9969\n",
      "Epoch 35/50\n",
      "13106/13106 [==============================] - 2s 190us/step - loss: 0.0182 - acc: 0.9956\n",
      "Epoch 36/50\n",
      "13106/13106 [==============================] - 3s 203us/step - loss: 0.0133 - acc: 0.9975\n",
      "Epoch 37/50\n",
      "13106/13106 [==============================] - 2s 184us/step - loss: 0.0126 - acc: 0.9974\n",
      "Epoch 38/50\n",
      "13106/13106 [==============================] - 3s 206us/step - loss: 0.0162 - acc: 0.9961\n",
      "Epoch 39/50\n",
      "13106/13106 [==============================] - 3s 198us/step - loss: 0.0153 - acc: 0.9960\n",
      "Epoch 40/50\n",
      "13106/13106 [==============================] - 2s 188us/step - loss: 0.0087 - acc: 0.9982\n",
      "Epoch 41/50\n",
      "13106/13106 [==============================] - 2s 180us/step - loss: 0.0091 - acc: 0.9979\n",
      "Epoch 42/50\n",
      "13106/13106 [==============================] - 2s 183us/step - loss: 0.0295 - acc: 0.9917\n",
      "Epoch 43/50\n",
      "13106/13106 [==============================] - 2s 185us/step - loss: 0.0086 - acc: 0.9982\n",
      "Epoch 44/50\n",
      "13106/13106 [==============================] - 2s 181us/step - loss: 0.0326 - acc: 0.9907\n",
      "Epoch 45/50\n",
      "13106/13106 [==============================] - 2s 184us/step - loss: 0.0220 - acc: 0.9936\n",
      "Epoch 46/50\n",
      "13106/13106 [==============================] - 3s 217us/step - loss: 0.0098 - acc: 0.9979\n",
      "Epoch 47/50\n",
      "13106/13106 [==============================] - 3s 223us/step - loss: 0.0080 - acc: 0.9984\n",
      "Epoch 48/50\n",
      "13106/13106 [==============================] - 3s 215us/step - loss: 0.0075 - acc: 0.9983\n",
      "Epoch 49/50\n",
      "13106/13106 [==============================] - 3s 200us/step - loss: 0.0234 - acc: 0.9935\n",
      "Epoch 50/50\n",
      "13106/13106 [==============================] - 3s 196us/step - loss: 0.0205 - acc: 0.9947\n",
      "5618/5618 [==============================] - 2s 270us/step\n",
      "Loss: 0.4285942423105175\n",
      "Accuracy: 0.9163403346386615\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 50\n",
    "printmd('**Building with new architecture...**')\n",
    "print('512(tanh) - 128(tanh) - 32(tanh) - 10(softmax)')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.tanh),\n",
    "    keras.layers.Dense(128, activation=tf.nn.tanh),\n",
    "    keras.layers.Dense(32, activation=tf.nn.tanh),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(lr), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом результаты не улучшились"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем добавить dropout (используем небольшой коэффициент, как рекомендуется здесь: https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "\n",
    "```Generally, use a small dropout value of 20%-50% of neurons with 20% providing a good starting point. A probability too low has minimal effect and a value too high results in under-learning by the network.```) и L2 регуляризатор (задачи L1 в целом решаются дропаутом):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Building with new architecture...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512(relu) - 128(relu) - 32(tanh) - 10(softmax)\n",
      "Learning rate: 0.1\n",
      "Epochs: 100\n",
      "\n",
      "Epoch 1/100\n",
      "13106/13106 [==============================] - 15s 1ms/step - loss: 5.3543 - acc: 0.7443\n",
      "Epoch 2/100\n",
      "13106/13106 [==============================] - 5s 396us/step - loss: 1.7255 - acc: 0.8274\n",
      "Epoch 3/100\n",
      "13106/13106 [==============================] - 5s 404us/step - loss: 1.0529 - acc: 0.8278\n",
      "Epoch 4/100\n",
      "13106/13106 [==============================] - 5s 398us/step - loss: 0.9096 - acc: 0.8287\n",
      "Epoch 5/100\n",
      "13106/13106 [==============================] - 6s 486us/step - loss: 0.8900 - acc: 0.8315\n",
      "Epoch 6/100\n",
      "13106/13106 [==============================] - 6s 471us/step - loss: 0.9015 - acc: 0.8264\n",
      "Epoch 7/100\n",
      "13106/13106 [==============================] - 6s 462us/step - loss: 0.9301 - acc: 0.8176\n",
      "Epoch 8/100\n",
      "13106/13106 [==============================] - 5s 393us/step - loss: 0.9137 - acc: 0.8237\n",
      "Epoch 9/100\n",
      "13106/13106 [==============================] - 6s 443us/step - loss: 0.9232 - acc: 0.8237\n",
      "Epoch 10/100\n",
      "13106/13106 [==============================] - 5s 417us/step - loss: 0.9070 - acc: 0.8231\n",
      "Epoch 11/100\n",
      "13106/13106 [==============================] - 6s 422us/step - loss: 0.8910 - acc: 0.8291\n",
      "Epoch 12/100\n",
      "13106/13106 [==============================] - 5s 414us/step - loss: 0.9151 - acc: 0.8228\n",
      "Epoch 13/100\n",
      "13106/13106 [==============================] - 5s 415us/step - loss: 0.8765 - acc: 0.8323\n",
      "Epoch 14/100\n",
      "13106/13106 [==============================] - 5s 395us/step - loss: 0.8928 - acc: 0.8250\n",
      "Epoch 15/100\n",
      "13106/13106 [==============================] - 5s 381us/step - loss: 0.8845 - acc: 0.8260\n",
      "Epoch 16/100\n",
      "13106/13106 [==============================] - 11s 811us/step - loss: 0.8916 - acc: 0.8245\n",
      "Epoch 17/100\n",
      "13106/13106 [==============================] - 6s 482us/step - loss: 0.9006 - acc: 0.8229\n",
      "Epoch 18/100\n",
      "13106/13106 [==============================] - 7s 497us/step - loss: 0.8866 - acc: 0.8258\n",
      "Epoch 19/100\n",
      "13106/13106 [==============================] - 5s 400us/step - loss: 0.8807 - acc: 0.8256\n",
      "Epoch 20/100\n",
      "13106/13106 [==============================] - 6s 452us/step - loss: 0.9023 - acc: 0.8202\n",
      "Epoch 21/100\n",
      "13106/13106 [==============================] - 5s 407us/step - loss: 0.9665 - acc: 0.8098\n",
      "Epoch 22/100\n",
      "13106/13106 [==============================] - 5s 415us/step - loss: 0.9149 - acc: 0.8241\n",
      "Epoch 23/100\n",
      "13106/13106 [==============================] - 5s 376us/step - loss: 0.9120 - acc: 0.8176\n",
      "Epoch 24/100\n",
      "13106/13106 [==============================] - 6s 420us/step - loss: 0.9884 - acc: 0.8038\n",
      "Epoch 25/100\n",
      "13106/13106 [==============================] - 6s 435us/step - loss: 0.9360 - acc: 0.8160\n",
      "Epoch 26/100\n",
      "13106/13106 [==============================] - 6s 425us/step - loss: 0.9343 - acc: 0.8154\n",
      "Epoch 27/100\n",
      "13106/13106 [==============================] - 5s 418us/step - loss: 0.9190 - acc: 0.8175\n",
      "Epoch 28/100\n",
      "13106/13106 [==============================] - 6s 425us/step - loss: 0.9510 - acc: 0.8142\n",
      "Epoch 29/100\n",
      "13106/13106 [==============================] - 5s 371us/step - loss: 0.9150 - acc: 0.8207\n",
      "Epoch 30/100\n",
      "13106/13106 [==============================] - 5s 375us/step - loss: 0.9257 - acc: 0.8176\n",
      "Epoch 31/100\n",
      "13106/13106 [==============================] - 6s 444us/step - loss: 0.9724 - acc: 0.8070\n",
      "Epoch 32/100\n",
      "13106/13106 [==============================] - 6s 444us/step - loss: 0.9337 - acc: 0.8184\n",
      "Epoch 33/100\n",
      "13106/13106 [==============================] - 5s 369us/step - loss: 0.9364 - acc: 0.8131\n",
      "Epoch 34/100\n",
      "13106/13106 [==============================] - 5s 380us/step - loss: 0.9599 - acc: 0.8088\n",
      "Epoch 35/100\n",
      "13106/13106 [==============================] - 7s 497us/step - loss: 0.9447 - acc: 0.8134\n",
      "Epoch 36/100\n",
      "13106/13106 [==============================] - 5s 399us/step - loss: 0.9636 - acc: 0.8096\n",
      "Epoch 37/100\n",
      "13106/13106 [==============================] - 5s 388us/step - loss: 0.9513 - acc: 0.8099\n",
      "Epoch 38/100\n",
      "13106/13106 [==============================] - 7s 499us/step - loss: 0.9158 - acc: 0.8163\n",
      "Epoch 39/100\n",
      "13106/13106 [==============================] - 7s 514us/step - loss: 1.0181 - acc: 0.7962\n",
      "Epoch 40/100\n",
      "13106/13106 [==============================] - 6s 453us/step - loss: 0.9611 - acc: 0.8078\n",
      "Epoch 41/100\n",
      "13106/13106 [==============================] - 6s 427us/step - loss: 0.9421 - acc: 0.8141\n",
      "Epoch 42/100\n",
      "13106/13106 [==============================] - 5s 389us/step - loss: 0.9642 - acc: 0.8068\n",
      "Epoch 43/100\n",
      "13106/13106 [==============================] - 7s 535us/step - loss: 1.0303 - acc: 0.7970\n",
      "Epoch 44/100\n",
      "13106/13106 [==============================] - 6s 458us/step - loss: 0.9772 - acc: 0.8061\n",
      "Epoch 45/100\n",
      "13106/13106 [==============================] - 5s 408us/step - loss: 0.9501 - acc: 0.8123\n",
      "Epoch 46/100\n",
      "13106/13106 [==============================] - 8s 599us/step - loss: 0.9409 - acc: 0.8118\n",
      "Epoch 47/100\n",
      "13106/13106 [==============================] - 7s 522us/step - loss: 0.9257 - acc: 0.8095\n",
      "Epoch 48/100\n",
      "13106/13106 [==============================] - 6s 474us/step - loss: 0.9366 - acc: 0.8144\n",
      "Epoch 49/100\n",
      "13106/13106 [==============================] - 6s 481us/step - loss: 0.9442 - acc: 0.8124\n",
      "Epoch 50/100\n",
      "13106/13106 [==============================] - 6s 424us/step - loss: 0.9478 - acc: 0.8050\n",
      "Epoch 51/100\n",
      "13106/13106 [==============================] - 6s 425us/step - loss: 0.9709 - acc: 0.8034\n",
      "Epoch 52/100\n",
      "13106/13106 [==============================] - 6s 424us/step - loss: 0.9862 - acc: 0.8012\n",
      "Epoch 53/100\n",
      "13106/13106 [==============================] - 6s 423us/step - loss: 1.0012 - acc: 0.7986\n",
      "Epoch 54/100\n",
      "13106/13106 [==============================] - 6s 422us/step - loss: 0.9259 - acc: 0.8166\n",
      "Epoch 55/100\n",
      "13106/13106 [==============================] - 6s 428us/step - loss: 0.9043 - acc: 0.8205\n",
      "Epoch 56/100\n",
      "13106/13106 [==============================] - 6s 426us/step - loss: 0.9471 - acc: 0.8092\n",
      "Epoch 57/100\n",
      "13106/13106 [==============================] - 6s 471us/step - loss: 0.9402 - acc: 0.8120\n",
      "Epoch 58/100\n",
      "13106/13106 [==============================] - 6s 429us/step - loss: 0.9616 - acc: 0.8063\n",
      "Epoch 59/100\n",
      "13106/13106 [==============================] - 6s 423us/step - loss: 0.9378 - acc: 0.8125\n",
      "Epoch 60/100\n",
      "13106/13106 [==============================] - 6s 427us/step - loss: 0.9314 - acc: 0.8096\n",
      "Epoch 61/100\n",
      "13106/13106 [==============================] - 5s 419us/step - loss: 0.9316 - acc: 0.8128\n",
      "Epoch 62/100\n",
      "13106/13106 [==============================] - 6s 424us/step - loss: 0.9848 - acc: 0.8020\n",
      "Epoch 63/100\n",
      "13106/13106 [==============================] - 6s 425us/step - loss: 1.0865 - acc: 0.7778\n",
      "Epoch 64/100\n",
      "13106/13106 [==============================] - 6s 425us/step - loss: 0.9783 - acc: 0.8005\n",
      "Epoch 65/100\n",
      "13106/13106 [==============================] - 6s 426us/step - loss: 1.0088 - acc: 0.7929\n",
      "Epoch 66/100\n",
      "13106/13106 [==============================] - 6s 423us/step - loss: 1.0012 - acc: 0.7994\n",
      "Epoch 67/100\n",
      "13106/13106 [==============================] - 6s 425us/step - loss: 0.9936 - acc: 0.8009\n",
      "Epoch 68/100\n",
      "13106/13106 [==============================] - 6s 440us/step - loss: 0.9644 - acc: 0.8076\n",
      "Epoch 69/100\n",
      "13106/13106 [==============================] - 6s 428us/step - loss: 0.9505 - acc: 0.8108\n",
      "Epoch 70/100\n",
      "13106/13106 [==============================] - 6s 437us/step - loss: 0.9375 - acc: 0.8083\n",
      "Epoch 71/100\n",
      "13106/13106 [==============================] - 6s 437us/step - loss: 0.9859 - acc: 0.7999\n",
      "Epoch 72/100\n",
      "13106/13106 [==============================] - 6s 428us/step - loss: 0.9833 - acc: 0.8004\n",
      "Epoch 73/100\n",
      "13106/13106 [==============================] - 6s 431us/step - loss: 0.9547 - acc: 0.8064\n",
      "Epoch 74/100\n",
      "13106/13106 [==============================] - 6s 428us/step - loss: 0.9651 - acc: 0.8067\n",
      "Epoch 75/100\n",
      "13106/13106 [==============================] - 6s 430us/step - loss: 0.9894 - acc: 0.7957\n",
      "Epoch 76/100\n",
      "13106/13106 [==============================] - 6s 435us/step - loss: 0.9503 - acc: 0.8102\n",
      "Epoch 77/100\n",
      "13106/13106 [==============================] - 6s 432us/step - loss: 0.9676 - acc: 0.8026\n",
      "Epoch 78/100\n",
      "13106/13106 [==============================] - 6s 427us/step - loss: 0.9580 - acc: 0.8094\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13106/13106 [==============================] - 5s 405us/step - loss: 0.9978 - acc: 0.7950\n",
      "Epoch 80/100\n",
      "13106/13106 [==============================] - 5s 403us/step - loss: 0.9354 - acc: 0.8121\n",
      "Epoch 81/100\n",
      "13106/13106 [==============================] - 5s 404us/step - loss: 0.9724 - acc: 0.7983\n",
      "Epoch 82/100\n",
      "13106/13106 [==============================] - 5s 403us/step - loss: 0.9574 - acc: 0.8059\n",
      "Epoch 83/100\n",
      "13106/13106 [==============================] - 5s 402us/step - loss: 1.0022 - acc: 0.7989\n",
      "Epoch 84/100\n",
      "13106/13106 [==============================] - 5s 404us/step - loss: 0.9917 - acc: 0.8019\n",
      "Epoch 85/100\n",
      "13106/13106 [==============================] - 5s 406us/step - loss: 1.0041 - acc: 0.7966\n",
      "Epoch 86/100\n",
      "13106/13106 [==============================] - 5s 403us/step - loss: 0.9859 - acc: 0.7978\n",
      "Epoch 87/100\n",
      "13106/13106 [==============================] - 5s 406us/step - loss: 0.9763 - acc: 0.8016\n",
      "Epoch 88/100\n",
      "13106/13106 [==============================] - 5s 415us/step - loss: 1.0177 - acc: 0.7883\n",
      "Epoch 89/100\n",
      "13106/13106 [==============================] - 5s 401us/step - loss: 0.9752 - acc: 0.8027\n",
      "Epoch 90/100\n",
      "13106/13106 [==============================] - 5s 411us/step - loss: 0.9762 - acc: 0.7985\n",
      "Epoch 91/100\n",
      "13106/13106 [==============================] - 5s 404us/step - loss: 0.9767 - acc: 0.7998\n",
      "Epoch 92/100\n",
      "13106/13106 [==============================] - 5s 407us/step - loss: 0.9800 - acc: 0.7963\n",
      "Epoch 93/100\n",
      "13106/13106 [==============================] - 5s 407us/step - loss: 1.0589 - acc: 0.7805\n",
      "Epoch 94/100\n",
      "13106/13106 [==============================] - 5s 406us/step - loss: 0.9756 - acc: 0.8004\n",
      "Epoch 95/100\n",
      "13106/13106 [==============================] - 5s 410us/step - loss: 0.9739 - acc: 0.8009\n",
      "Epoch 96/100\n",
      "13106/13106 [==============================] - 5s 408us/step - loss: 0.9832 - acc: 0.7993\n",
      "Epoch 97/100\n",
      "13106/13106 [==============================] - 5s 410us/step - loss: 1.0203 - acc: 0.7912\n",
      "Epoch 98/100\n",
      "13106/13106 [==============================] - 5s 406us/step - loss: 1.0285 - acc: 0.7891\n",
      "Epoch 99/100\n",
      "13106/13106 [==============================] - 5s 413us/step - loss: 1.0003 - acc: 0.7943\n",
      "Epoch 100/100\n",
      "13106/13106 [==============================] - 5s 412us/step - loss: 1.0681 - acc: 0.7783\n",
      "5618/5618 [==============================] - 3s 614us/step\n",
      "Loss: 0.9961611226004781\n",
      "Accuracy: 0.783730864991663\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 100\n",
    "printmd('**Building with new architecture...**')\n",
    "print('512(relu) - 128(relu) - 32(tanh) - 10(softmax)')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(32, activation=tf.nn.tanh, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(lr), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без регуляризатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Building with new architecture...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512(relu) - 128(relu) - 32(tanh) - 10(softmax)\n",
      "Learning rate: 0.1\n",
      "Epochs: 100\n",
      "\n",
      "Epoch 1/100\n",
      "13106/13106 [==============================] - 7s 502us/step - loss: 0.8214 - acc: 0.7533\n",
      "Epoch 2/100\n",
      "13106/13106 [==============================] - 3s 255us/step - loss: 0.5010 - acc: 0.8535\n",
      "Epoch 3/100\n",
      "13106/13106 [==============================] - 3s 253us/step - loss: 0.4362 - acc: 0.8668\n",
      "Epoch 4/100\n",
      "13106/13106 [==============================] - 3s 252us/step - loss: 0.4007 - acc: 0.8796\n",
      "Epoch 5/100\n",
      "13106/13106 [==============================] - 3s 252us/step - loss: 0.3711 - acc: 0.8884\n",
      "Epoch 6/100\n",
      "13106/13106 [==============================] - 3s 257us/step - loss: 0.3521 - acc: 0.8920\n",
      "Epoch 7/100\n",
      "13106/13106 [==============================] - 3s 253us/step - loss: 0.3368 - acc: 0.8978\n",
      "Epoch 8/100\n",
      "13106/13106 [==============================] - 3s 256us/step - loss: 0.3156 - acc: 0.9018\n",
      "Epoch 9/100\n",
      "13106/13106 [==============================] - 3s 264us/step - loss: 0.3123 - acc: 0.9040\n",
      "Epoch 10/100\n",
      "13106/13106 [==============================] - 3s 257us/step - loss: 0.2908 - acc: 0.9111\n",
      "Epoch 11/100\n",
      "13106/13106 [==============================] - 3s 260us/step - loss: 0.2804 - acc: 0.9126\n",
      "Epoch 12/100\n",
      "13106/13106 [==============================] - 3s 262us/step - loss: 0.2751 - acc: 0.9147\n",
      "Epoch 13/100\n",
      "13106/13106 [==============================] - 3s 261us/step - loss: 0.2599 - acc: 0.9183\n",
      "Epoch 14/100\n",
      "13106/13106 [==============================] - 3s 258us/step - loss: 0.2589 - acc: 0.9184\n",
      "Epoch 15/100\n",
      "13106/13106 [==============================] - 3s 264us/step - loss: 0.2470 - acc: 0.9217\n",
      "Epoch 16/100\n",
      "13106/13106 [==============================] - 3s 265us/step - loss: 0.2482 - acc: 0.9206\n",
      "Epoch 17/100\n",
      "13106/13106 [==============================] - 3s 263us/step - loss: 0.2358 - acc: 0.9235\n",
      "Epoch 18/100\n",
      "13106/13106 [==============================] - 3s 263us/step - loss: 0.2223 - acc: 0.9278\n",
      "Epoch 19/100\n",
      "13106/13106 [==============================] - 3s 264us/step - loss: 0.2254 - acc: 0.9270\n",
      "Epoch 20/100\n",
      "13106/13106 [==============================] - 3s 266us/step - loss: 0.2129 - acc: 0.9304\n",
      "Epoch 21/100\n",
      "13106/13106 [==============================] - 3s 266us/step - loss: 0.2078 - acc: 0.9317\n",
      "Epoch 22/100\n",
      "13106/13106 [==============================] - 3s 267us/step - loss: 0.2060 - acc: 0.9329\n",
      "Epoch 23/100\n",
      "13106/13106 [==============================] - 4s 267us/step - loss: 0.2009 - acc: 0.9340 1s - loss:\n",
      "Epoch 24/100\n",
      "13106/13106 [==============================] - 4s 275us/step - loss: 0.1973 - acc: 0.9362\n",
      "Epoch 25/100\n",
      "13106/13106 [==============================] - 4s 269us/step - loss: 0.1865 - acc: 0.9402\n",
      "Epoch 26/100\n",
      "13106/13106 [==============================] - 4s 270us/step - loss: 0.1825 - acc: 0.9404\n",
      "Epoch 27/100\n",
      "13106/13106 [==============================] - 4s 272us/step - loss: 0.1831 - acc: 0.9396\n",
      "Epoch 28/100\n",
      "13106/13106 [==============================] - 4s 270us/step - loss: 0.1784 - acc: 0.9429\n",
      "Epoch 29/100\n",
      "13106/13106 [==============================] - 4s 273us/step - loss: 0.1743 - acc: 0.9432\n",
      "Epoch 30/100\n",
      "13106/13106 [==============================] - 4s 277us/step - loss: 0.1722 - acc: 0.9442\n",
      "Epoch 31/100\n",
      "13106/13106 [==============================] - 4s 271us/step - loss: 0.1661 - acc: 0.9445\n",
      "Epoch 32/100\n",
      "13106/13106 [==============================] - 4s 271us/step - loss: 0.1668 - acc: 0.9470\n",
      "Epoch 33/100\n",
      "13106/13106 [==============================] - 4s 272us/step - loss: 0.1566 - acc: 0.9501\n",
      "Epoch 34/100\n",
      "13106/13106 [==============================] - 4s 294us/step - loss: 0.1486 - acc: 0.9514\n",
      "Epoch 35/100\n",
      "13106/13106 [==============================] - 4s 281us/step - loss: 0.1548 - acc: 0.9472\n",
      "Epoch 36/100\n",
      "13106/13106 [==============================] - 4s 274us/step - loss: 0.1543 - acc: 0.9486\n",
      "Epoch 37/100\n",
      "13106/13106 [==============================] - 4s 273us/step - loss: 0.1504 - acc: 0.9507\n",
      "Epoch 38/100\n",
      "13106/13106 [==============================] - 4s 275us/step - loss: 0.1419 - acc: 0.9537\n",
      "Epoch 39/100\n",
      "13106/13106 [==============================] - 4s 274us/step - loss: 0.1352 - acc: 0.9556\n",
      "Epoch 40/100\n",
      "13106/13106 [==============================] - 4s 271us/step - loss: 0.1392 - acc: 0.9544 0s - loss: 0.1403 \n",
      "Epoch 41/100\n",
      "13106/13106 [==============================] - 4s 271us/step - loss: 0.1381 - acc: 0.9569\n",
      "Epoch 42/100\n",
      "13106/13106 [==============================] - 4s 273us/step - loss: 0.1390 - acc: 0.9541\n",
      "Epoch 43/100\n",
      "13106/13106 [==============================] - 4s 272us/step - loss: 0.1312 - acc: 0.9572\n",
      "Epoch 44/100\n",
      "13106/13106 [==============================] - 4s 271us/step - loss: 0.1158 - acc: 0.9605\n",
      "Epoch 45/100\n",
      "13106/13106 [==============================] - 4s 271us/step - loss: 0.1286 - acc: 0.9576\n",
      "Epoch 46/100\n",
      "13106/13106 [==============================] - 4s 277us/step - loss: 0.1326 - acc: 0.9573\n",
      "Epoch 47/100\n",
      "13106/13106 [==============================] - 4s 275us/step - loss: 0.1327 - acc: 0.9557\n",
      "Epoch 48/100\n",
      "13106/13106 [==============================] - 4s 272us/step - loss: 0.1220 - acc: 0.9599\n",
      "Epoch 49/100\n",
      "13106/13106 [==============================] - 4s 271us/step - loss: 0.1234 - acc: 0.9577\n",
      "Epoch 50/100\n",
      "13106/13106 [==============================] - 4s 272us/step - loss: 0.1132 - acc: 0.9637\n",
      "Epoch 51/100\n",
      "13106/13106 [==============================] - 4s 268us/step - loss: 0.1141 - acc: 0.9630\n",
      "Epoch 52/100\n",
      "13106/13106 [==============================] - 4s 271us/step - loss: 0.1196 - acc: 0.9610\n",
      "Epoch 53/100\n",
      "13106/13106 [==============================] - 4s 268us/step - loss: 0.1140 - acc: 0.9622\n",
      "Epoch 54/100\n",
      "13106/13106 [==============================] - 4s 274us/step - loss: 0.1192 - acc: 0.9628\n",
      "Epoch 55/100\n",
      "13106/13106 [==============================] - 4s 270us/step - loss: 0.1117 - acc: 0.9614\n",
      "Epoch 56/100\n",
      "13106/13106 [==============================] - 4s 268us/step - loss: 0.1019 - acc: 0.9669\n",
      "Epoch 57/100\n",
      "13106/13106 [==============================] - 4s 269us/step - loss: 0.1032 - acc: 0.9653\n",
      "Epoch 58/100\n",
      "13106/13106 [==============================] - 4s 270us/step - loss: 0.1021 - acc: 0.9667\n",
      "Epoch 59/100\n",
      "13106/13106 [==============================] - 4s 273us/step - loss: 0.1058 - acc: 0.9644\n",
      "Epoch 60/100\n",
      "13106/13106 [==============================] - 4s 270us/step - loss: 0.1065 - acc: 0.9644\n",
      "Epoch 61/100\n",
      "13106/13106 [==============================] - 4s 269us/step - loss: 0.0952 - acc: 0.9665\n",
      "Epoch 62/100\n",
      "13106/13106 [==============================] - 4s 269us/step - loss: 0.1003 - acc: 0.9676\n",
      "Epoch 63/100\n",
      "13106/13106 [==============================] - 4s 270us/step - loss: 0.0997 - acc: 0.9660\n",
      "Epoch 64/100\n",
      "13106/13106 [==============================] - 4s 282us/step - loss: 0.0971 - acc: 0.9683\n",
      "Epoch 65/100\n",
      "13106/13106 [==============================] - 4s 272us/step - loss: 0.0930 - acc: 0.9678\n",
      "Epoch 66/100\n",
      "13106/13106 [==============================] - 4s 272us/step - loss: 0.0915 - acc: 0.9696\n",
      "Epoch 67/100\n",
      "13106/13106 [==============================] - 4s 270us/step - loss: 0.1051 - acc: 0.9650\n",
      "Epoch 68/100\n",
      "13106/13106 [==============================] - 4s 270us/step - loss: 0.0865 - acc: 0.9713\n",
      "Epoch 69/100\n",
      "13106/13106 [==============================] - 4s 274us/step - loss: 0.0926 - acc: 0.9689\n",
      "Epoch 70/100\n",
      "13106/13106 [==============================] - 4s 272us/step - loss: 0.0874 - acc: 0.9700\n",
      "Epoch 71/100\n",
      "13106/13106 [==============================] - 4s 276us/step - loss: 0.0806 - acc: 0.9743\n",
      "Epoch 72/100\n",
      "13106/13106 [==============================] - 4s 273us/step - loss: 0.0873 - acc: 0.9705\n",
      "Epoch 73/100\n",
      "13106/13106 [==============================] - 4s 272us/step - loss: 0.0880 - acc: 0.9701\n",
      "Epoch 74/100\n",
      "13106/13106 [==============================] - 4s 273us/step - loss: 0.0821 - acc: 0.9736\n",
      "Epoch 75/100\n",
      "13106/13106 [==============================] - 4s 278us/step - loss: 0.0895 - acc: 0.9710\n",
      "Epoch 76/100\n",
      "13106/13106 [==============================] - 4s 273us/step - loss: 0.0834 - acc: 0.9719\n",
      "Epoch 77/100\n",
      "13106/13106 [==============================] - 4s 274us/step - loss: 0.0832 - acc: 0.9731\n",
      "Epoch 78/100\n",
      "13106/13106 [==============================] - 4s 273us/step - loss: 0.0906 - acc: 0.9712\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13106/13106 [==============================] - 3s 255us/step - loss: 0.0713 - acc: 0.9769\n",
      "Epoch 80/100\n",
      "13106/13106 [==============================] - 3s 254us/step - loss: 0.0788 - acc: 0.9741\n",
      "Epoch 81/100\n",
      "13106/13106 [==============================] - 3s 255us/step - loss: 0.0800 - acc: 0.9741\n",
      "Epoch 82/100\n",
      "13106/13106 [==============================] - 3s 255us/step - loss: 0.0786 - acc: 0.9733\n",
      "Epoch 83/100\n",
      "13106/13106 [==============================] - 4s 268us/step - loss: 0.0773 - acc: 0.9742\n",
      "Epoch 84/100\n",
      "13106/13106 [==============================] - 3s 257us/step - loss: 0.0718 - acc: 0.9766\n",
      "Epoch 85/100\n",
      "13106/13106 [==============================] - 4s 296us/step - loss: 0.0712 - acc: 0.9771\n",
      "Epoch 86/100\n",
      "13106/13106 [==============================] - 3s 265us/step - loss: 0.0781 - acc: 0.9731\n",
      "Epoch 87/100\n",
      "13106/13106 [==============================] - 3s 257us/step - loss: 0.0729 - acc: 0.9759\n",
      "Epoch 88/100\n",
      "13106/13106 [==============================] - 3s 258us/step - loss: 0.0721 - acc: 0.9772\n",
      "Epoch 89/100\n",
      "13106/13106 [==============================] - 3s 256us/step - loss: 0.0759 - acc: 0.9753\n",
      "Epoch 90/100\n",
      "13106/13106 [==============================] - 3s 255us/step - loss: 0.0679 - acc: 0.9773\n",
      "Epoch 91/100\n",
      "13106/13106 [==============================] - 3s 255us/step - loss: 0.0617 - acc: 0.9797\n",
      "Epoch 92/100\n",
      "13106/13106 [==============================] - 3s 256us/step - loss: 0.0688 - acc: 0.9789\n",
      "Epoch 93/100\n",
      "13106/13106 [==============================] - 3s 255us/step - loss: 0.0645 - acc: 0.9780\n",
      "Epoch 94/100\n",
      "13106/13106 [==============================] - 3s 257us/step - loss: 0.0806 - acc: 0.9741\n",
      "Epoch 95/100\n",
      "13106/13106 [==============================] - 3s 257us/step - loss: 0.0716 - acc: 0.9763\n",
      "Epoch 96/100\n",
      "13106/13106 [==============================] - 3s 256us/step - loss: 0.0705 - acc: 0.9776\n",
      "Epoch 97/100\n",
      "13106/13106 [==============================] - 3s 255us/step - loss: 0.0635 - acc: 0.9793\n",
      "Epoch 98/100\n",
      "13106/13106 [==============================] - 3s 262us/step - loss: 0.0640 - acc: 0.9786\n",
      "Epoch 99/100\n",
      "13106/13106 [==============================] - 3s 260us/step - loss: 0.0639 - acc: 0.9783\n",
      "Epoch 100/100\n",
      "13106/13106 [==============================] - 3s 259us/step - loss: 0.0602 - acc: 0.9800\n",
      "5618/5618 [==============================] - 2s 289us/step\n",
      "Loss: 0.3621504976871144\n",
      "Accuracy: 0.9220363118547525\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 100\n",
    "printmd('**Building with new architecture...**')\n",
    "print('512(relu) - 128(relu) - 32(tanh) - 10(softmax)')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(32, activation=tf.nn.tanh),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(lr), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, регуляризатор лишь ухудшил результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем применить динамический (адаптивный) коэффициент обучения. Для этого воспользуемся методом `Adam`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Building with new architecture...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512(relu) - 128(relu) - 32(tanh) - 10(softmax)\n",
      "Learning rate: 0.001\n",
      "Epochs: 100\n",
      "\n",
      "Epoch 1/100\n",
      "13106/13106 [==============================] - 10s 749us/step - loss: 0.8558 - acc: 0.7429\n",
      "Epoch 2/100\n",
      "13106/13106 [==============================] - 7s 536us/step - loss: 0.5364 - acc: 0.8405\n",
      "Epoch 3/100\n",
      "13106/13106 [==============================] - 7s 514us/step - loss: 0.4724 - acc: 0.8618\n",
      "Epoch 4/100\n",
      "13106/13106 [==============================] - 7s 508us/step - loss: 0.4617 - acc: 0.8620\n",
      "Epoch 5/100\n",
      "13106/13106 [==============================] - 7s 505us/step - loss: 0.4279 - acc: 0.8694\n",
      "Epoch 6/100\n",
      "13106/13106 [==============================] - 7s 515us/step - loss: 0.4076 - acc: 0.8763\n",
      "Epoch 7/100\n",
      "13106/13106 [==============================] - 7s 510us/step - loss: 0.4032 - acc: 0.8761\n",
      "Epoch 8/100\n",
      "13106/13106 [==============================] - 7s 501us/step - loss: 0.3940 - acc: 0.8801\n",
      "Epoch 9/100\n",
      "13106/13106 [==============================] - 7s 518us/step - loss: 0.3720 - acc: 0.8872\n",
      "Epoch 10/100\n",
      "13106/13106 [==============================] - 8s 608us/step - loss: 0.3704 - acc: 0.8884\n",
      "Epoch 11/100\n",
      "13106/13106 [==============================] - 6s 483us/step - loss: 0.3703 - acc: 0.8843\n",
      "Epoch 12/100\n",
      "13106/13106 [==============================] - 10s 770us/step - loss: 0.3495 - acc: 0.8932\n",
      "Epoch 13/100\n",
      "13106/13106 [==============================] - 6s 483us/step - loss: 0.3601 - acc: 0.8897\n",
      "Epoch 14/100\n",
      "13106/13106 [==============================] - 6s 457us/step - loss: 0.3415 - acc: 0.8969\n",
      "Epoch 15/100\n",
      "13106/13106 [==============================] - 6s 464us/step - loss: 0.3354 - acc: 0.8993\n",
      "Epoch 16/100\n",
      "13106/13106 [==============================] - 6s 472us/step - loss: 0.3367 - acc: 0.8944\n",
      "Epoch 17/100\n",
      "13106/13106 [==============================] - 6s 450us/step - loss: 0.3381 - acc: 0.8944\n",
      "Epoch 18/100\n",
      "13106/13106 [==============================] - 6s 459us/step - loss: 0.3262 - acc: 0.8971\n",
      "Epoch 19/100\n",
      "13106/13106 [==============================] - 6s 442us/step - loss: 0.3120 - acc: 0.9029\n",
      "Epoch 20/100\n",
      "13106/13106 [==============================] - 7s 503us/step - loss: 0.3151 - acc: 0.9026\n",
      "Epoch 21/100\n",
      "13106/13106 [==============================] - 8s 593us/step - loss: 0.2985 - acc: 0.9063\n",
      "Epoch 22/100\n",
      "13106/13106 [==============================] - 6s 477us/step - loss: 0.3035 - acc: 0.9055\n",
      "Epoch 23/100\n",
      "13106/13106 [==============================] - 6s 489us/step - loss: 0.2980 - acc: 0.9081\n",
      "Epoch 24/100\n",
      "13106/13106 [==============================] - 6s 489us/step - loss: 0.3000 - acc: 0.9047\n",
      "Epoch 25/100\n",
      "13106/13106 [==============================] - 7s 499us/step - loss: 0.2989 - acc: 0.9068\n",
      "Epoch 26/100\n",
      "13106/13106 [==============================] - 7s 533us/step - loss: 0.2925 - acc: 0.9068\n",
      "Epoch 27/100\n",
      "13106/13106 [==============================] - 6s 494us/step - loss: 0.2846 - acc: 0.9118\n",
      "Epoch 28/100\n",
      "13106/13106 [==============================] - 6s 485us/step - loss: 0.2922 - acc: 0.9089\n",
      "Epoch 29/100\n",
      "13106/13106 [==============================] - 6s 495us/step - loss: 0.2858 - acc: 0.9090\n",
      "Epoch 30/100\n",
      "13106/13106 [==============================] - 9s 707us/step - loss: 0.2791 - acc: 0.9113\n",
      "Epoch 31/100\n",
      "13106/13106 [==============================] - 6s 476us/step - loss: 0.2774 - acc: 0.9136\n",
      "Epoch 32/100\n",
      "13106/13106 [==============================] - 6s 490us/step - loss: 0.2793 - acc: 0.9138\n",
      "Epoch 33/100\n",
      "13106/13106 [==============================] - 6s 444us/step - loss: 0.2761 - acc: 0.9122\n",
      "Epoch 34/100\n",
      "13106/13106 [==============================] - 6s 483us/step - loss: 0.2668 - acc: 0.9121\n",
      "Epoch 35/100\n",
      "13106/13106 [==============================] - 8s 645us/step - loss: 0.2547 - acc: 0.9189\n",
      "Epoch 36/100\n",
      "13106/13106 [==============================] - 8s 621us/step - loss: 0.2684 - acc: 0.9136\n",
      "Epoch 37/100\n",
      "13106/13106 [==============================] - 7s 528us/step - loss: 0.2597 - acc: 0.9175\n",
      "Epoch 38/100\n",
      "13106/13106 [==============================] - 7s 515us/step - loss: 0.2606 - acc: 0.9177\n",
      "Epoch 39/100\n",
      "13106/13106 [==============================] - 7s 528us/step - loss: 0.2573 - acc: 0.9162\n",
      "Epoch 40/100\n",
      "13106/13106 [==============================] - 7s 537us/step - loss: 0.2625 - acc: 0.9152\n",
      "Epoch 41/100\n",
      "13106/13106 [==============================] - 8s 577us/step - loss: 0.2632 - acc: 0.9170\n",
      "Epoch 42/100\n",
      "13106/13106 [==============================] - 8s 616us/step - loss: 0.2497 - acc: 0.9205\n",
      "Epoch 43/100\n",
      "13106/13106 [==============================] - 7s 524us/step - loss: 0.2594 - acc: 0.9168\n",
      "Epoch 44/100\n",
      "13106/13106 [==============================] - 8s 585us/step - loss: 0.2513 - acc: 0.9224\n",
      "Epoch 45/100\n",
      "13106/13106 [==============================] - 7s 508us/step - loss: 0.2469 - acc: 0.9204\n",
      "Epoch 46/100\n",
      "13106/13106 [==============================] - 6s 444us/step - loss: 0.2523 - acc: 0.9178\n",
      "Epoch 47/100\n",
      "13106/13106 [==============================] - 6s 443us/step - loss: 0.2510 - acc: 0.9201\n",
      "Epoch 48/100\n",
      "13106/13106 [==============================] - 6s 450us/step - loss: 0.2445 - acc: 0.9222\n",
      "Epoch 49/100\n",
      "13106/13106 [==============================] - 6s 439us/step - loss: 0.2428 - acc: 0.9202\n",
      "Epoch 50/100\n",
      "13106/13106 [==============================] - 6s 439us/step - loss: 0.2446 - acc: 0.9227\n",
      "Epoch 51/100\n",
      "13106/13106 [==============================] - 6s 474us/step - loss: 0.2530 - acc: 0.9180\n",
      "Epoch 52/100\n",
      "13106/13106 [==============================] - 6s 447us/step - loss: 0.2437 - acc: 0.9206\n",
      "Epoch 53/100\n",
      "13106/13106 [==============================] - 6s 471us/step - loss: 0.2484 - acc: 0.9196\n",
      "Epoch 54/100\n",
      "13106/13106 [==============================] - 6s 466us/step - loss: 0.2425 - acc: 0.9226\n",
      "Epoch 55/100\n",
      "13106/13106 [==============================] - 7s 561us/step - loss: 0.2324 - acc: 0.9247\n",
      "Epoch 56/100\n",
      "13106/13106 [==============================] - 6s 492us/step - loss: 0.2175 - acc: 0.9299\n",
      "Epoch 57/100\n",
      "13106/13106 [==============================] - 6s 484us/step - loss: 0.2236 - acc: 0.9287\n",
      "Epoch 58/100\n",
      "13106/13106 [==============================] - 7s 555us/step - loss: 0.2238 - acc: 0.9289\n",
      "Epoch 59/100\n",
      "13106/13106 [==============================] - 7s 546us/step - loss: 0.2368 - acc: 0.9229\n",
      "Epoch 60/100\n",
      "13106/13106 [==============================] - 7s 538us/step - loss: 0.2315 - acc: 0.9255\n",
      "Epoch 61/100\n",
      "13106/13106 [==============================] - 6s 474us/step - loss: 0.2290 - acc: 0.9254\n",
      "Epoch 62/100\n",
      "13106/13106 [==============================] - 7s 531us/step - loss: 0.2279 - acc: 0.9280\n",
      "Epoch 63/100\n",
      "13106/13106 [==============================] - 7s 533us/step - loss: 0.2209 - acc: 0.9300\n",
      "Epoch 64/100\n",
      "13106/13106 [==============================] - 9s 656us/step - loss: 0.2297 - acc: 0.9261\n",
      "Epoch 65/100\n",
      "13106/13106 [==============================] - 6s 450us/step - loss: 0.2269 - acc: 0.9283\n",
      "Epoch 66/100\n",
      "13106/13106 [==============================] - 6s 478us/step - loss: 0.2234 - acc: 0.9279\n",
      "Epoch 67/100\n",
      "13106/13106 [==============================] - 6s 435us/step - loss: 0.2261 - acc: 0.9264\n",
      "Epoch 68/100\n",
      "13106/13106 [==============================] - 6s 433us/step - loss: 0.2188 - acc: 0.9309\n",
      "Epoch 69/100\n",
      "13106/13106 [==============================] - 6s 435us/step - loss: 0.2090 - acc: 0.9309\n",
      "Epoch 70/100\n",
      "13106/13106 [==============================] - 6s 440us/step - loss: 0.2143 - acc: 0.9297\n",
      "Epoch 71/100\n",
      "13106/13106 [==============================] - 6s 441us/step - loss: 0.2124 - acc: 0.9310\n",
      "Epoch 72/100\n",
      "13106/13106 [==============================] - 6s 436us/step - loss: 0.2186 - acc: 0.9287\n",
      "Epoch 73/100\n",
      "13106/13106 [==============================] - 6s 436us/step - loss: 0.2149 - acc: 0.9288\n",
      "Epoch 74/100\n",
      "13106/13106 [==============================] - 6s 433us/step - loss: 0.2108 - acc: 0.9316\n",
      "Epoch 75/100\n",
      "13106/13106 [==============================] - 6s 434us/step - loss: 0.2149 - acc: 0.9303\n",
      "Epoch 76/100\n",
      "13106/13106 [==============================] - 6s 437us/step - loss: 0.2099 - acc: 0.9328\n",
      "Epoch 77/100\n",
      "13106/13106 [==============================] - 6s 434us/step - loss: 0.2135 - acc: 0.9322\n",
      "Epoch 78/100\n",
      "13106/13106 [==============================] - 6s 434us/step - loss: 0.1985 - acc: 0.9364\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13106/13106 [==============================] - 6s 434us/step - loss: 0.2133 - acc: 0.9303\n",
      "Epoch 80/100\n",
      "13106/13106 [==============================] - 6s 439us/step - loss: 0.2110 - acc: 0.9311\n",
      "Epoch 81/100\n",
      "13106/13106 [==============================] - 6s 435us/step - loss: 0.2158 - acc: 0.9307\n",
      "Epoch 82/100\n",
      "13106/13106 [==============================] - 6s 435us/step - loss: 0.2186 - acc: 0.9284\n",
      "Epoch 83/100\n",
      "13106/13106 [==============================] - 7s 508us/step - loss: 0.2128 - acc: 0.9299\n",
      "Epoch 84/100\n",
      "13106/13106 [==============================] - 6s 436us/step - loss: 0.2102 - acc: 0.9316\n",
      "Epoch 85/100\n",
      "13106/13106 [==============================] - 6s 435us/step - loss: 0.2078 - acc: 0.9329\n",
      "Epoch 86/100\n",
      "13106/13106 [==============================] - 6s 435us/step - loss: 0.2143 - acc: 0.9306\n",
      "Epoch 87/100\n",
      "13106/13106 [==============================] - 6s 432us/step - loss: 0.2033 - acc: 0.9345\n",
      "Epoch 88/100\n",
      "13106/13106 [==============================] - 6s 442us/step - loss: 0.2133 - acc: 0.9317\n",
      "Epoch 89/100\n",
      "13106/13106 [==============================] - 6s 438us/step - loss: 0.2067 - acc: 0.9347\n",
      "Epoch 90/100\n",
      "13106/13106 [==============================] - 6s 449us/step - loss: 0.2078 - acc: 0.9303\n",
      "Epoch 91/100\n",
      "13106/13106 [==============================] - 6s 435us/step - loss: 0.1970 - acc: 0.9340\n",
      "Epoch 92/100\n",
      "13106/13106 [==============================] - 6s 434us/step - loss: 0.2172 - acc: 0.9298\n",
      "Epoch 93/100\n",
      "13106/13106 [==============================] - 6s 482us/step - loss: 0.1910 - acc: 0.9387\n",
      "Epoch 94/100\n",
      "13106/13106 [==============================] - 6s 479us/step - loss: 0.1878 - acc: 0.9373\n",
      "Epoch 95/100\n",
      "13106/13106 [==============================] - 7s 527us/step - loss: 0.1971 - acc: 0.9354\n",
      "Epoch 96/100\n",
      "13106/13106 [==============================] - 8s 574us/step - loss: 0.2093 - acc: 0.9318\n",
      "Epoch 97/100\n",
      "13106/13106 [==============================] - 7s 530us/step - loss: 0.1943 - acc: 0.9387\n",
      "Epoch 98/100\n",
      "13106/13106 [==============================] - 8s 588us/step - loss: 0.1960 - acc: 0.9355\n",
      "Epoch 99/100\n",
      "13106/13106 [==============================] - 9s 669us/step - loss: 0.1909 - acc: 0.9375\n",
      "Epoch 100/100\n",
      "13106/13106 [==============================] - 7s 541us/step - loss: 0.2020 - acc: 0.9332\n",
      "5618/5618 [==============================] - 3s 500us/step\n",
      "Loss: 0.35621672610269217\n",
      "Accuracy: 0.912246351014596\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "epochs = 100\n",
    "printmd('**Building with new architecture...**')\n",
    "print('512(relu) - 128(relu) - 32(tanh) - 10(softmax)')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.tanh),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "loss_method = 'sparse_categorical_crossentropy'\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(lr), \n",
    "              loss=loss_method,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Building with new architecture...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512(relu) - 128(relu) - 32(tanh) - 10(softmax)\n",
      "Learning rate: 0.01\n",
      "Epochs: 100\n",
      "\n",
      "Epoch 1/100\n",
      "13106/13106 [==============================] - 9s 711us/step - loss: 2.3248 - acc: 0.1008\n",
      "Epoch 2/100\n",
      "13106/13106 [==============================] - 8s 596us/step - loss: 2.3068 - acc: 0.0989\n",
      "Epoch 3/100\n",
      "13106/13106 [==============================] - 6s 473us/step - loss: 2.3044 - acc: 0.0942\n",
      "Epoch 4/100\n",
      "13106/13106 [==============================] - 6s 478us/step - loss: 2.3041 - acc: 0.0970\n",
      "Epoch 5/100\n",
      "13106/13106 [==============================] - 6s 477us/step - loss: 2.3053 - acc: 0.0968\n",
      "Epoch 6/100\n",
      "13106/13106 [==============================] - 7s 497us/step - loss: 2.3054 - acc: 0.0979\n",
      "Epoch 7/100\n",
      " 7008/13106 [===============>..............] - ETA: 3s - loss: 2.3038 - acc: 0.1059"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-92c73645490a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loss: {loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "epochs = 100\n",
    "printmd('**Building with new architecture...**')\n",
    "print('512(relu) - 128(relu) - 32(tanh) - 10(softmax)')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {epochs}')\n",
    "print()\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.tanh),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "loss_method = 'sparse_categorical_crossentropy'\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(lr, beta1=0.8, beta2=0.9), \n",
    "              loss=loss_method,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print('=======================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> @chiselko6 </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
